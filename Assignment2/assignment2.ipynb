{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b933634",
   "metadata": {
    "tags": []
   },
   "source": [
    "# IDS Assignment Part 2 - <font color=\"red\"><h7>Deadline: 23/01/2023 23:59</h7></font>\n",
    "This is the second part of the assignment in IDS 2022/2023. \n",
    "Please use this Jupyter notebook to work on the questions posed in the assignment. When you are done, upload the notebook in Moodle at the designated activity. In addition to the _Jupyter notebook_, please submit _one zip-file_ containing your screenshots for Question 7. \n",
    "\n",
    "Give your commented Python code and answers in the corresponding provided cells. Make sure to answer all questions in a clear and explicit manner and discuss your outputs. _Please do not change the general structure of this notebook_. You can, however, add additional markdown or code cells if necessary. <b>Please DO NOT CLEAR THE OUTPUT of the notebook you are submitting! </b>\n",
    "\n",
    "<font color=\"red\"> *Please make sure to include the names and matriculation numbers of all group members in the slot provided below.* </font> If a name or a student id is missing, the student will not receive any points.\n",
    "\n",
    "Hint 1: While working on the assignment, you will get a better understanding of the dataset. Feel free to generate additional results and visualizations to support your answers. For example, this might be useful regarding data modification, data simplification, or output interpretation. <font color=\"red\">Ensure that all your claims are supported.</font>\n",
    "\n",
    "Hint 2: <font color=\"red\">Plan your time wisely. </font> A few parts of this assignment may take some time to run. It might be necessary to consider time management when you plan your group work. Also, do not attempt to upload your assignment at the last minute before the deadline. This often does not work, and you will miss the deadline. Late submissions will not be considered.\n",
    "\n",
    "Hint 3: RWTHmoodle allows multiple submissions, with every new submission overwriting the previous one. <b>Partial submissions are therefore possible and encouraged. </b> This might be helpful in case of technical issues with RWTHMoodle, which may occur close to the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db0f12",
   "metadata": {},
   "source": [
    "<font color=\"red\"><b>Student Names and IDs:\n",
    "    \n",
    "    1. \n",
    "    \n",
    "    2. \n",
    "    \n",
    "    3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7803d7da-69e1-412a-8a9f-c04ceb96a0f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 1: Preprocessing (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c5e97f-99a5-41d4-92a8-e9888bf4f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "### Sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffab9bb-d08f-44dd-ad3f-ac3dc8e2184c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "In this question, we consider a dataset documenting the Ski Resorts in Europe (**ski_resorts.csv**).\n",
    "Each row contains some information about the Ski resort.\n",
    "You can find a short description for each column:\n",
    "\n",
    "| Column | Description |\n",
    "| ------ | ----------- |\n",
    "| Resort | The name of the ski & snowboard resort. |\n",
    "| Country | The country in which the resort is located. |\n",
    "| HighestPoint | The highest mountain point at the ski resort.   |\n",
    "| LowestPoint | The lowest possible point to ski at the ski resort.  |\n",
    "| DayPassPriceAdult | The price shows what it costs for 1 adult for 1 day in the main season in Euro. |\n",
    "| BeginnerSlope | The total amount of “beginner” slopes in kilometer at the resort. “Beginner slopes” contains “children”, “blue” and, “green” slopes. |\n",
    "| IntermediateSlope | The total amount of “intermediate” slopes in kilometer at the resort. “Intermediate slopes” contains “red” slopes. |\n",
    "| DifficultSlope | The total amount of “difficult” slopes in kilometer at the resort. “Difficult slopes” contains “black”, “advanced”, and ”expert” slopes. |\n",
    "| TotalSlope | The sum of “beginner slopes” + “intermediate slopes” + “difficult slopes” |\n",
    "| Snowparks | Does the resort have one or more snowparks, or not? |\n",
    "| NightSki | Does the resort offer skiing on illuminated slopes? |\n",
    "| SurfaceLifts | The amount of lifts in this category: T-bar, Sunkidslift, Rope lifts, and people mower. |\n",
    "| ChairLifts | The total amount of chairlifts. |\n",
    "| GondolaLifts | The amount of lifts in this category: Gondola, Train lifts, Funicular, Combined gondola and chairlifts, Helicopter lifts, Snowcats, and Aerial tramways. |\n",
    "| TotalLifts | The sum of “surface lifts etc” + “gondola etc” + “chairlifts etc.” |\n",
    "| LiftCapacity | How many passengers can the lift system at the ski resort mowe in one hour? |\n",
    "| SnowCannons  |The total amount of snow cannons at the ski resort.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7bd4f6-32a5-4e9d-9339-00556a268506",
   "metadata": {
    "tags": []
   },
   "source": [
    "### a) Loading the Data and Initial Quality Investigation (2.5pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c39c84-0399-4369-ad4d-c657c08aefba",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **a(i)** \n",
    "Load the dataset into a dataframe `df`. <font color='red'>Use the first column as index for your dataframe</font>. Ensure that the index is valid, that is, it should not contain any duplicate entries. \n",
    "\n",
    "\n",
    "\n",
    "**In the subsequent questions, only modify the dataframe `df` if explicitly requested. However, you can always create working copies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897a95f5-c50d-4240-b357-65ff937034ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b69ca-8603-4cf9-8e6a-32bea8b70a32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T10:01:29.281703Z",
     "iopub.status.busy": "2022-12-07T10:01:29.281301Z",
     "iopub.status.idle": "2022-12-07T10:01:29.305867Z",
     "shell.execute_reply": "2022-12-07T10:01:29.304586Z",
     "shell.execute_reply.started": "2022-12-07T10:01:29.281673Z"
    },
    "tags": []
   },
   "source": [
    "#### **a(ii)** \n",
    "Show the data types of the dataframe columns as well as the first 5 rows. On the first sight, are there any data type problems (e.g., numerical columns having a non-numerical data type)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2e18e6-f590-449d-891f-f5684fdb23e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34febc32-6c1a-45ec-96c5-f81dda43ba7b",
   "metadata": {},
   "source": [
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ff406b-5ec1-4b26-8abb-5b5a91643841",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **a(iii)** \n",
    "To improve performance and memory usage (in particular for large datasets) it is important to use **categorical** columns whenever suitable.\n",
    "Are there any categorical column candidates? Explain your answer. \\\n",
    "Afterward, convert the column(s) in `df` into categorical column(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa75081-bf63-4e85-94c2-483d9ae4118e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c3379-bb44-42d0-9c4f-9d1e4c2d38b6",
   "metadata": {},
   "source": [
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222a54c-f455-4b78-bae1-33bc7d79edf3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:58:43.496952Z",
     "iopub.status.busy": "2022-12-07T16:58:43.496547Z",
     "iopub.status.idle": "2022-12-07T16:58:43.506339Z",
     "shell.execute_reply": "2022-12-07T16:58:43.504499Z",
     "shell.execute_reply.started": "2022-12-07T16:58:43.496921Z"
    },
    "tags": []
   },
   "source": [
    "### b) Handling Missing Values & Encoding (17.5pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279b17dc-acae-4012-b83b-2bae628a8fb4",
   "metadata": {},
   "source": [
    "In the following task, you can assume that every NAN entry in the dataframe is actually a missing value. This can partially be justified by the fact that pandas did not have problems inferring the \"proper\" datatypes (e.g., a string indicating a missing number in a number column would result in pandas parsing an object column) and your subsequent check of the data types. Therefore, you can use `df.isna()` as a proxy indicator for missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a760130-8c4b-46b6-b96f-76dcea95d28e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-07T16:59:35.630762Z",
     "iopub.status.busy": "2022-12-07T16:59:35.630357Z",
     "iopub.status.idle": "2022-12-07T16:59:35.648425Z",
     "shell.execute_reply": "2022-12-07T16:59:35.646703Z",
     "shell.execute_reply.started": "2022-12-07T16:59:35.630730Z"
    },
    "tags": []
   },
   "source": [
    "#### **b(i)** \n",
    "Simply discarding missing entries is usually not a good idea. Therefore, you should first analyze the number of missing values and check for patterns of missing values. \n",
    "\n",
    "To this end, compute the following statistics on missing values:\n",
    "1. How many entries does the dataframe have? (To relate this to the number of entries missing)\n",
    "2. How many missing values do we have? What is the ratio i.e., \"number of missing values\"/\"number of entries of df\"?\n",
    "3. How many rows have at least a single missing value?\n",
    "4. Count the number of missing values per column.\n",
    "5. Count the number of missing values per row and aggregate them - i.e., show the number of rows that suffer from x missing values.\n",
    "6. What do you observe? Are there any rows containing missing values for the same set of columns? Can you identify potential patterns?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b7c69e-8c4e-483e-9131-9b7f2936216c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code for 1. How many entries does the dataframe have? (To relate this to the number of entries missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10f80a7-4caf-4d0b-8cf8-c3148e466288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code for 2. How many missing values do we have? What is the ratio i.e., \"number of missing values\"/\"number of entries of df\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ecc58b-6563-4dd1-8727-4aa08a485961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code for 3. How many rows have at least a single missing value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e8a812-67c1-4458-ac9e-41265cab70c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code for 4. Count the number of missing values per column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724f6928-edfe-4fec-b3fd-abfc6567d1ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code for 5. Count the number of missing values per row and aggregate them - i.e., show the number of rows that suffer from x missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc8a782-1430-4e7c-835c-91b7e1806597",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Your answer:** *(for 6. What do you observe? Are there any rows containing missing values for the same set of columns?)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5467e51c-6ba1-47f6-a123-fd55076b8e45",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(ii)**\n",
    "For the next step:\n",
    "\n",
    "1. Transform the categorical column(s) you identified in a(iii) into one-hot encoding format. \n",
    "2. Transform the columns \"Snowparks\" and \"NightSki\" in `df` into boolean data type, where \"Yes\" should be `True` and \"No\" should be `False`\n",
    "\n",
    "In the end, the original categorical column(s) should still be there. Additionally, there should be x number (x is the number of unique values) of one-hot encoding columns for each categorical column. Use the following naming convention for the new columns \"{name of the categorical column}_{unique value for that column}\" Also, make sure the columns \"Snowparks\" and \"NightSki\" are boolean type in the end.\n",
    "Lastly, print the top five rows of the resulting dataframe.\n",
    "\n",
    "*Hint: You can use the pd.get_dummies() function from pandas for the first transformation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4098a7f-2640-4577-9fe3-09e0bf7519b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for 1. Transform the categorical column(s) you identified in a(iii) into one-hot encoding format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d6bff-270d-4e5a-93e7-62dc97b0551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for 2. Transform the columns \"Snowparks\" and \"NightSki\" in `df` into boolean data type, where \"Yes\" should be `True` and \"No\" should be `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02638dbf-36cd-47d8-9eee-07591eeb5ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use df.dtypes to check if you correctly transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3192a2-dedb-45ab-b5c0-86b49a7a15e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(iii)** \n",
    "The previous analysis in b(i) showed that there are missing values in the 'SurfaceLifts' and 'GondolaLifts' columns.\\\n",
    "How would you impute these values? \\\n",
    "Motivate your approach and apply it to `df`.\n",
    "\n",
    "*Hint: Remember the semantics of the columns. Also, carefully assert your assumptions.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e34370-d15a-4f57-8df1-b11ae3e4b9a5",
   "metadata": {},
   "source": [
    "**Your Answer:** *(Motivate your approach.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07628a31-b88b-4692-92ca-2aec93f36173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a15869a-3c62-4363-9248-012b1ae373d0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(iv)**\n",
    "Impute the rest of the missing values using the knn-imputation method. To this end, apply the following steps:\n",
    "1. Create a working copy `df_tmp` of your updated `df`.\n",
    "2. For simplicity, drop the non-numerical columns (i.e., not of types integer or floats), which also inlcude the one-hot encoded and the boolean columns* you created earlier.\n",
    "3. Normalize the data in `df_tmp` (e.g., Standard score normalization). If the features have very different scales, knn can become very biased.\n",
    "4. Impute the missing values considering six neighbors.\n",
    "5. Invert the transformation applied upfront to enable more meaningful and intuitive visualizations.\n",
    "6. Append the columns you dropped in step 2.\n",
    " \n",
    "In the end, `df` should not contain missing values and have all the columns.\n",
    "\n",
    "\\*Note that by dropping the columns we lose the information of countries and the two boolean attributes (\"Snowparks\" and \"NightSki\") when imputing the missing values, which might be crucial for inferencing values such as the price for a ski pass. In practice, one should try to find if there are correlations before deciding whether to drop the columns or not.\n",
    "We drop the columns here to make the following steps easier because we only have to deal with numerical columns.\n",
    "\n",
    "*Hint: Be careful with the indices of your dataframes.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc58ba-2699-4826-9dda-f0c5c135f32e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code for step 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7600aaaf-a864-47bb-aa62-76bfd644fd53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code for step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce24019-3dba-438d-a7ea-dcc3244bbc53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code for step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42034302-c720-4f4d-9512-10bfa9842d70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code for step 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de8f607-4016-4a8c-966e-0bcd3ee086b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# your code for step 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5dd094-f496-4a98-81fb-1e941ee90472",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# assert df.isna().sum().sum() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52685085-a3d3-4a40-a979-069ec578c01e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 2: Visualization (13 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fdcbdb-792d-4c66-b5ef-4e86d6a955b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "In this task, you will analyze the data using different means of visualization.\n",
    "\n",
    "Start with the following preprocessed and integrated dataframe `df_v`. \\\n",
    "Note that it has a similar structure to the dataframe that you should obtain from the previous task, however, the values have been modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620155fc-f0c9-4f49-b778-6379c949bb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570ecd50-427b-4203-bb2f-f2fcbd1052a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v = pd.read_csv(\"./datasets/ski_resorts_visual.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de753a8-98a9-4336-9e91-0acc1c73cf60",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **a) (3pts)** \n",
    "To start the visual analysis, make a Scatter plot matrix to visually check if there are any correlations between the numerical attributes.\n",
    "\n",
    "*Hint: You can use the scatter_matrix from pandas.plotting or pairplot from seaborn to make the plot.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a7b99e-a13f-42b0-862b-91968efe7542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60f31f7-cba3-4c6d-be6f-fdefb5c6fb17",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **b) (3pts)**\n",
    "Another way to detect correlation is to calculate the Pearson correlation coefficient. Calculate the correlation matrix for the numerical data and visualize the matrix using a heatmap. \n",
    "Briefly discuss your findings from the heatmap and the scatter plot you created in 2(a).\n",
    "\n",
    "Make sure to annotate the heatmap with the values of the correlation.\n",
    "\n",
    "*Hint: You can use the heatmap function from seaborn to make the plot.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd3d2ce-7a30-4fa6-a5e9-46cefd79e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f387c1b8-4e34-4b68-88bf-774048366104",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **c) (4pts)** \n",
    "We now want to have an overview of the attribute \"TotalSlope\" aggregated by different levels of hierarchy (Europe -> Country -> Resort). It seems that a tree map is suitable for this purpose.\n",
    "\n",
    "Make a tree map where\n",
    "- the root node represents Europe.\n",
    "- the child nodes of Europe are countries.\n",
    "- the child nodes of each country are the ski resorts.\n",
    "- the size of the rectangles is determined by the attribute \"TotalSlope\".\n",
    "\n",
    "Also, use the tree map to find out\n",
    "1. The sum of TotalSlopes of a country, list the top five countries and the corresponding values.\n",
    "2. The max value of TotalSlope of the five countries you identified in 1.\n",
    "\n",
    "*Hint: You can use the treemap function from plotly.express.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed294b7-8822-45fe-adfc-4a4e4d024124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6279cedc-ab47-4bc1-ac36-bdac4fb91f2b",
   "metadata": {},
   "source": [
    "**Your answer for...** \\\n",
    "*...  1. The sum of TotalSlopes of a country. List the top five countries and the corresponding values:* \\\n",
    "*...  2. The max value of TotalSlope of the five countries you identified in 1:*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd91227-df95-4371-8538-fac931bc4745",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **d) (3pts)** \n",
    "\n",
    "An alternative to a tree map is a sunburst plot, the principle is similar to a tree map. \n",
    "\n",
    "Recall from the lecture that:\n",
    "- Each ring is a different level of the hierarchy\n",
    "- Each segment of a ring belongs to one categorical value\n",
    "- The size of a segment is either divided proportionally to a value\n",
    "\n",
    "Now, we would like to have an overview of the attribute \"TotalLifts\" aggregated by different level of hierarchy.\n",
    "\n",
    "Make a sunburst plot where\n",
    "- the first hierarchy(ring) is \"Country\"\n",
    "- the second hierarchy(ring) is \"Snowparks\" (whether the resort has snowparks)\n",
    "- the third hierarchy(ring) is \"Resort\"\n",
    "- the size of the segments is determined by the attribute \"TotalLifts\".\n",
    "\n",
    "Then, briefly discuss your findings from the plot.\n",
    "\n",
    "*Hint: You can use the sunburst function from plotly.express.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7203fa15-8f8b-4cb7-b836-de047bb6a2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24702c9-93a5-47d9-88ef-cbd018ae840b",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eb1982-1fa7-4829-9dfa-df0922f8de3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 3 - Frequent Item Sets and Association Rules (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc7d780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules as arule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e7e462-5ead-4531-a306-7bf6047647d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### A real online retail transaction data set of two years.\n",
    "\n",
    "Data Set Information:\n",
    "This Retail dataset contains all the transactions occurring for a UK-based and registered, non-store online retail between 01/12/2009 and 09/12/2011.The company mainly sells unique all-occasion gift ware. Many customers of the company are wholesalers.\n",
    "\n",
    "Attribute Information:\n",
    "- Invoice: Invoice number. Nominal. A 6-digit integral number is uniquely assigned to each transaction. If the number starts with 'C' it refers to a canceled transaction.\n",
    "- StockCode: Product (item) code. Nominal. A 5-digit integral number uniquely assigned to each distinct product.\n",
    "- Description: Product (item) name. Nominal.\n",
    "- Quantity: The quantities of each product (item) per transaction. Numeric.\n",
    "- InvoiceDate: Invoice date and time. Numeric. The day and time when a transaction was generated.\n",
    "- Price: Unit price. Numeric. Product price per unit in sterling (£).\n",
    "- CustomerID: Customer number. Nominal. A 5-digit integral number is uniquely assigned to each customer. This number has postfix 'n'.\n",
    "- Country: Country name. Nominal. The name of the country where a customer resides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9794fa4d-e5c8-4699-be1b-389dbd32b9c4",
   "metadata": {},
   "source": [
    "### a) Loading, exploring and preprocessing the dataset (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a2d2b7-4dda-4a52-a597-80fdfaae628f",
   "metadata": {},
   "source": [
    "#### **a(i)** \n",
    "Load the data from `retail.csv` and save it under the variable `retail_df`. Display the first few lines of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc75ad8d-88bb-4ef3-ab32-ae5a1f38954e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Invoice</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>Price</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536365</td>\n",
       "      <td>85123A</td>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "      <td>6</td>\n",
       "      <td>01/12/2010 08:26</td>\n",
       "      <td>2.55</td>\n",
       "      <td>178500</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536365</td>\n",
       "      <td>71053</td>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "      <td>6</td>\n",
       "      <td>01/12/2010 08:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>178500</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536365</td>\n",
       "      <td>84406B</td>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "      <td>8</td>\n",
       "      <td>01/12/2010 08:26</td>\n",
       "      <td>2.75</td>\n",
       "      <td>178500</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029G</td>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "      <td>6</td>\n",
       "      <td>01/12/2010 08:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>178500</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029E</td>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "      <td>6</td>\n",
       "      <td>01/12/2010 08:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>178500</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Invoice StockCode                          Description  Quantity  \\\n",
       "0  536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
       "1  536365     71053                  WHITE METAL LANTERN         6   \n",
       "2  536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
       "3  536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
       "4  536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
       "\n",
       "        InvoiceDate  Price  CustomerID         Country  \n",
       "0  01/12/2010 08:26   2.55      178500  United Kingdom  \n",
       "1  01/12/2010 08:26   3.39      178500  United Kingdom  \n",
       "2  01/12/2010 08:26   2.75      178500  United Kingdom  \n",
       "3  01/12/2010 08:26   3.39      178500  United Kingdom  \n",
       "4  01/12/2010 08:26   3.39      178500  United Kingdom  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "retail_df = pd.read_csv('./datasets/retail.csv')\n",
    "retail_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba06313-e9f1-43cd-ab08-07e56458234b",
   "metadata": {},
   "source": [
    "#### **a(ii)** \n",
    "To get to know the dataset, do the following:\n",
    "\n",
    "- Show the number of rows in the dataset.\n",
    "- Show the number of unique customers.\n",
    "- Show the number of unique product names.\n",
    "- Show the number of unique invoices.\n",
    "- Show the number and the list of all the countries where the customers reside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eadfbf62-1e6b-4b5d-9556-46d9c2fc2614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 406525\n",
      "Number of Unique Customers: 4367\n",
      "Number of Unique Product Names: 3684\n",
      "Number of Unique Product Names: 3896\n",
      "Number of Unique Invoices: 22177\n",
      "Number of Countries: 35\n",
      "Names of Countries: ['United Kingdom' 'France' 'Australia' 'Netherlands' 'Germany' 'Norway'\n",
      " 'EIRE' 'Switzerland' 'Spain' 'Poland' 'Portugal' 'Italy' 'Belgium'\n",
      " 'Lithuania' 'Japan' 'Iceland' 'Channel Islands' 'Denmark' 'Cyprus'\n",
      " 'Sweden' 'Austria' 'Israel' 'Finland' 'Greece' 'Singapore' 'Lebanon'\n",
      " 'United Arab Emirates' 'Saudi Arabia' 'Czech Republic' 'Canada' 'Brazil'\n",
      " 'USA' 'Bahrain' 'Malta' 'RSA']\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "print(\"Number of Rows: {}\".format(retail_df.shape[0]))\n",
    "print(\"Number of Unique Customers: {}\".format(len(retail_df[\"CustomerID\"].unique())))\n",
    "print(\"Number of Unique Product Names: {}\".format(len(retail_df[\"StockCode\"].unique())))\n",
    "print(\"Number of Unique Product Names: {}\".format(len(retail_df[\"Description\"].unique())))\n",
    "print(\"Number of Unique Invoices: {}\".format(len(retail_df[\"Invoice\"].unique())))\n",
    "print(\"Number of Countries: {}\".format(len(retail_df[\"Country\"].unique())))\n",
    "print(\"Names of Countries: {}\".format(retail_df[\"Country\"].unique()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071fc555-19fd-4488-8e2c-efdb0cb785be",
   "metadata": {},
   "source": [
    "#### **a(iii)** \n",
    "You are interested in analyzing itemsets that are frequently purchased together. Before continuing with that task, you have to make sure that the data are fit for such analysis. 1) More precisely, you want to make sure that there are no missing values in the data. 2) Moreover, you want to ensure that each item's name in the \"Description\" is consistent. E.g., you want \"Description\" values such as \" coffee black\", \"coffee &nbsp;black\", \" coffee black &nbsp;\", etc. to be mapped to the same value (e.g. \"coffee black\"). 3) Last but not least, you want to remove transactions that were canceled. Such transactions correspond to rows where the invoice number starts with letter 'C'.\n",
    "\n",
    "Apply these preprocessing steps to the dataset `retail_df` and apply them on the dataframe itself (e.g. set inplace=True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bb891d0-6362-47c5-903d-4caf7333a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "\n",
    "retail_df.dropna()\n",
    "retail_df[\"Description\"] = retail_df[\"Description\"].map(lambda x: ' '.join(\"{}\".format(x).strip().split()))\n",
    "retail_df = retail_df[~retail_df['Invoice'].str.startswith('C')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2226dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Invoice</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>Price</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536365</td>\n",
       "      <td>85123A</td>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "      <td>6</td>\n",
       "      <td>01/12/2010 08:26</td>\n",
       "      <td>2.55</td>\n",
       "      <td>178500</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536365</td>\n",
       "      <td>71053</td>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "      <td>6</td>\n",
       "      <td>01/12/2010 08:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>178500</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536365</td>\n",
       "      <td>84406B</td>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "      <td>8</td>\n",
       "      <td>01/12/2010 08:26</td>\n",
       "      <td>2.75</td>\n",
       "      <td>178500</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029G</td>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "      <td>6</td>\n",
       "      <td>01/12/2010 08:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>178500</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029E</td>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "      <td>6</td>\n",
       "      <td>01/12/2010 08:26</td>\n",
       "      <td>3.39</td>\n",
       "      <td>178500</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406520</th>\n",
       "      <td>581587</td>\n",
       "      <td>22899</td>\n",
       "      <td>CHILDREN'S APRON DOLLY GIRL</td>\n",
       "      <td>6</td>\n",
       "      <td>09/12/2011 12:50</td>\n",
       "      <td>2.10</td>\n",
       "      <td>126800</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406521</th>\n",
       "      <td>581587</td>\n",
       "      <td>23254</td>\n",
       "      <td>CHILDRENS CUTLERY DOLLY GIRL</td>\n",
       "      <td>4</td>\n",
       "      <td>09/12/2011 12:50</td>\n",
       "      <td>4.15</td>\n",
       "      <td>126800</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406522</th>\n",
       "      <td>581587</td>\n",
       "      <td>23255</td>\n",
       "      <td>CHILDRENS CUTLERY CIRCUS PARADE</td>\n",
       "      <td>4</td>\n",
       "      <td>09/12/2011 12:50</td>\n",
       "      <td>4.15</td>\n",
       "      <td>126800</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406523</th>\n",
       "      <td>581587</td>\n",
       "      <td>22138</td>\n",
       "      <td>BAKING SET 9 PIECE RETROSPOT</td>\n",
       "      <td>3</td>\n",
       "      <td>09/12/2011 12:50</td>\n",
       "      <td>4.95</td>\n",
       "      <td>126800</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406524</th>\n",
       "      <td>581587</td>\n",
       "      <td>POST</td>\n",
       "      <td>POSTAGE</td>\n",
       "      <td>1</td>\n",
       "      <td>09/12/2011 12:50</td>\n",
       "      <td>18.00</td>\n",
       "      <td>126800</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>397621 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Invoice StockCode                          Description  Quantity  \\\n",
       "0       536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
       "1       536365     71053                  WHITE METAL LANTERN         6   \n",
       "2       536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
       "3       536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
       "4       536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
       "...        ...       ...                                  ...       ...   \n",
       "406520  581587     22899          CHILDREN'S APRON DOLLY GIRL         6   \n",
       "406521  581587     23254         CHILDRENS CUTLERY DOLLY GIRL         4   \n",
       "406522  581587     23255      CHILDRENS CUTLERY CIRCUS PARADE         4   \n",
       "406523  581587     22138         BAKING SET 9 PIECE RETROSPOT         3   \n",
       "406524  581587      POST                              POSTAGE         1   \n",
       "\n",
       "             InvoiceDate  Price  CustomerID         Country  \n",
       "0       01/12/2010 08:26   2.55      178500  United Kingdom  \n",
       "1       01/12/2010 08:26   3.39      178500  United Kingdom  \n",
       "2       01/12/2010 08:26   2.75      178500  United Kingdom  \n",
       "3       01/12/2010 08:26   3.39      178500  United Kingdom  \n",
       "4       01/12/2010 08:26   3.39      178500  United Kingdom  \n",
       "...                  ...    ...         ...             ...  \n",
       "406520  09/12/2011 12:50   2.10      126800          France  \n",
       "406521  09/12/2011 12:50   4.15      126800          France  \n",
       "406522  09/12/2011 12:50   4.15      126800          France  \n",
       "406523  09/12/2011 12:50   4.95      126800          France  \n",
       "406524  09/12/2011 12:50  18.00      126800          France  \n",
       "\n",
       "[397621 rows x 8 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retail_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb88ce-11d8-405f-94b8-a0d670da853a",
   "metadata": {},
   "source": [
    "#### **a(iv)** \n",
    "After applying the preprocessing steps in **a(iii)** , repeat again the task **a(ii)**, that is:\n",
    "\n",
    "- Show the number of rows in the dataset.\n",
    "- Show the number of unique customers.\n",
    "- Show the number of all unique product names.\n",
    "- Show the list of all the countries where the customers reside.\n",
    "\n",
    "Which values changed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ff1aab7-1048-4043-989b-cd97437f0ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows: 397621\n",
      "Number of Unique Customers: 4334\n",
      "Number of Unique Product Names: 3665\n",
      "Number of Unique Product Names: 3858\n",
      "Number of Countries: 35\n",
      "Names of Countries: ['United Kingdom' 'France' 'Australia' 'Netherlands' 'Germany' 'Norway'\n",
      " 'EIRE' 'Switzerland' 'Spain' 'Poland' 'Portugal' 'Italy' 'Belgium'\n",
      " 'Lithuania' 'Japan' 'Iceland' 'Channel Islands' 'Denmark' 'Cyprus'\n",
      " 'Sweden' 'Finland' 'Austria' 'Greece' 'Singapore' 'Lebanon'\n",
      " 'United Arab Emirates' 'Israel' 'Saudi Arabia' 'Czech Republic' 'Canada'\n",
      " 'Brazil' 'USA' 'Bahrain' 'Malta' 'RSA']\n"
     ]
    }
   ],
   "source": [
    "# your code\n",
    "print(\"Number of Rows: {}\".format(retail_df.shape[0]))\n",
    "print(\"Number of Unique Customers: {}\".format(len(retail_df[\"CustomerID\"].unique())))\n",
    "print(\"Number of Unique Product Names: {}\".format(len(retail_df[\"StockCode\"].unique())))\n",
    "print(\"Number of Unique Product Names: {}\".format(len(retail_df[\"Description\"].unique())))\n",
    "print(\"Number of Countries: {}\".format(len(retail_df[\"Country\"].unique())))\n",
    "print(\"Names of Countries: {}\".format(retail_df[\"Country\"].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ace216-572c-47d9-977f-52c62d2cb26e",
   "metadata": {},
   "source": [
    "**Your answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105a9af7-a3c1-4d19-8cd8-778276106c52",
   "metadata": {},
   "source": [
    "### b) Frequent itemsets and Association rules (8 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d55a65f-a5a3-4616-8365-d6a99f0b4386",
   "metadata": {},
   "source": [
    "#### **b(i)** \n",
    "Each invoice number in the dataset identifies a unique transaction. There are potentially many rows in the dataframe having the same invoice number. We want to analyze items that are frequently purchased together, that is, items that appear in the same transaction.\n",
    "\n",
    "Create a new dataframe named `transaction_df` with two columns: \"Invoice\" and \"Description\". Here the \"Invoice\" value is the index of the dataframe (the unique number identifying each row) and \"Description\" is the column containing all items (without duplicates) purchased within the transaction with that invoice number. Display the first few rows of your dataframe. How many rows does the `transaction_df` have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec046573-e55d-4ea4-a9c6-7f9c82d8d5cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Invoice</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536365</td>\n",
       "      <td>[WHITE HANGING HEART T-LIGHT HOLDER, CREAM CUP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536366</td>\n",
       "      <td>[HAND WARMER UNION JACK, HAND WARMER RED POLKA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536367</td>\n",
       "      <td>[IVORY KNITTED MUG COSY, RECIPE BOX WITH METAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536368</td>\n",
       "      <td>[BLUE COAT RACK PARIS FASHION, JAM MAKING SET ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536369</td>\n",
       "      <td>[BATH BUILDING BLOCK WORD]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18519</th>\n",
       "      <td>581583</td>\n",
       "      <td>[LUNCH BAG RED RETROSPOT, 6 CHOCOLATE LOVE HEA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18520</th>\n",
       "      <td>581584</td>\n",
       "      <td>[6 CHOCOLATE LOVE HEART T-LIGHTS, RED FLOCK LO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18521</th>\n",
       "      <td>581585</td>\n",
       "      <td>[LOVE HOT WATER BOTTLE, EMBOSSED GLASS TEALIGH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18522</th>\n",
       "      <td>581586</td>\n",
       "      <td>[DOORMAT RED RETROSPOT, SET OF 3 HANGING OWLS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18523</th>\n",
       "      <td>581587</td>\n",
       "      <td>[ALARM CLOCK BAKELIKE IVORY, PLASTERS IN TIN C...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18524 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Invoice                                        Description\n",
       "0      536365  [WHITE HANGING HEART T-LIGHT HOLDER, CREAM CUP...\n",
       "1      536366  [HAND WARMER UNION JACK, HAND WARMER RED POLKA...\n",
       "2      536367  [IVORY KNITTED MUG COSY, RECIPE BOX WITH METAL...\n",
       "3      536368  [BLUE COAT RACK PARIS FASHION, JAM MAKING SET ...\n",
       "4      536369                         [BATH BUILDING BLOCK WORD]\n",
       "...       ...                                                ...\n",
       "18519  581583  [LUNCH BAG RED RETROSPOT, 6 CHOCOLATE LOVE HEA...\n",
       "18520  581584  [6 CHOCOLATE LOVE HEART T-LIGHTS, RED FLOCK LO...\n",
       "18521  581585  [LOVE HOT WATER BOTTLE, EMBOSSED GLASS TEALIGH...\n",
       "18522  581586  [DOORMAT RED RETROSPOT, SET OF 3 HANGING OWLS ...\n",
       "18523  581587  [ALARM CLOCK BAKELIKE IVORY, PLASTERS IN TIN C...\n",
       "\n",
       "[18524 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "# retail_df.groupby(\"Invoice\").head()\n",
    "transaction_df = retail_df.groupby('Invoice')['Description'].apply(list).reset_index(name='Description')\n",
    "# transaction_df.head()\n",
    "transaction_df['Description'] = transaction_df['Description'].map(lambda x: list(set(x)))\n",
    "transaction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f7615d-f7f2-4881-b081-23505c0875d6",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abcf0c1-0445-4701-874f-d7f8f600075f",
   "metadata": {},
   "source": [
    "#### **b(ii)** \n",
    "Next, we want to compute frequent itemsets and association rules based on the sets of items ordered together. Use the TransactionEncoder to transform `transaction_df` into a matrix such that the value in the i-th row and the j-th column is $True$ if the i-th itemset contains product j, and $False$ otherwise. Save the matrix into a dataframe named `transactions`. Display the shape of the matrix.\n",
    "\n",
    "*Hint: Note that your dataframe 'transactions' must contain as many rows as there are invoice numbers and as many columns as there are unique products.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5c0b0e0-46c9-4b8d-873a-1d0d1136ebdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10 COLOUR SPACEBOY PEN</th>\n",
       "      <th>12 COLOURED PARTY BALLOONS</th>\n",
       "      <th>12 DAISY PEGS IN WOOD BOX</th>\n",
       "      <th>12 EGG HOUSE PAINTED WOOD</th>\n",
       "      <th>12 HANGING EGGS HAND PAINTED</th>\n",
       "      <th>12 IVORY ROSE PEG PLACE SETTINGS</th>\n",
       "      <th>12 MESSAGE CARDS WITH ENVELOPES</th>\n",
       "      <th>12 PENCIL SMALL TUBE WOODLAND</th>\n",
       "      <th>12 PENCILS SMALL TUBE RED RETROSPOT</th>\n",
       "      <th>12 PENCILS SMALL TUBE SKULL</th>\n",
       "      <th>...</th>\n",
       "      <th>ZINC STAR T-LIGHT HOLDER</th>\n",
       "      <th>ZINC SWEETHEART SOAP DISH</th>\n",
       "      <th>ZINC SWEETHEART WIRE LETTER RACK</th>\n",
       "      <th>ZINC T-LIGHT HOLDER STAR LARGE</th>\n",
       "      <th>ZINC T-LIGHT HOLDER STARS LARGE</th>\n",
       "      <th>ZINC T-LIGHT HOLDER STARS SMALL</th>\n",
       "      <th>ZINC TOP 2 DOOR WOODEN SHELF</th>\n",
       "      <th>ZINC WILLIE WINKIE CANDLE STICK</th>\n",
       "      <th>ZINC WIRE KITCHEN ORGANISER</th>\n",
       "      <th>ZINC WIRE SWEETHEART LETTER TRAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18519</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18520</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18521</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18522</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18523</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18524 rows × 3858 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       10 COLOUR SPACEBOY PEN  12 COLOURED PARTY BALLOONS  \\\n",
       "0                       False                       False   \n",
       "1                       False                       False   \n",
       "2                       False                       False   \n",
       "3                       False                       False   \n",
       "4                       False                       False   \n",
       "...                       ...                         ...   \n",
       "18519                   False                       False   \n",
       "18520                   False                       False   \n",
       "18521                   False                       False   \n",
       "18522                   False                       False   \n",
       "18523                   False                       False   \n",
       "\n",
       "       12 DAISY PEGS IN WOOD BOX  12 EGG HOUSE PAINTED WOOD  \\\n",
       "0                          False                      False   \n",
       "1                          False                      False   \n",
       "2                          False                      False   \n",
       "3                          False                      False   \n",
       "4                          False                      False   \n",
       "...                          ...                        ...   \n",
       "18519                      False                      False   \n",
       "18520                      False                      False   \n",
       "18521                      False                      False   \n",
       "18522                      False                      False   \n",
       "18523                      False                      False   \n",
       "\n",
       "       12 HANGING EGGS HAND PAINTED  12 IVORY ROSE PEG PLACE SETTINGS  \\\n",
       "0                             False                             False   \n",
       "1                             False                             False   \n",
       "2                             False                             False   \n",
       "3                             False                             False   \n",
       "4                             False                             False   \n",
       "...                             ...                               ...   \n",
       "18519                         False                             False   \n",
       "18520                         False                             False   \n",
       "18521                         False                             False   \n",
       "18522                         False                             False   \n",
       "18523                         False                             False   \n",
       "\n",
       "       12 MESSAGE CARDS WITH ENVELOPES  12 PENCIL SMALL TUBE WOODLAND  \\\n",
       "0                                False                          False   \n",
       "1                                False                          False   \n",
       "2                                False                          False   \n",
       "3                                False                          False   \n",
       "4                                False                          False   \n",
       "...                                ...                            ...   \n",
       "18519                            False                          False   \n",
       "18520                            False                          False   \n",
       "18521                            False                          False   \n",
       "18522                            False                          False   \n",
       "18523                            False                          False   \n",
       "\n",
       "       12 PENCILS SMALL TUBE RED RETROSPOT  12 PENCILS SMALL TUBE SKULL  ...  \\\n",
       "0                                    False                        False  ...   \n",
       "1                                    False                        False  ...   \n",
       "2                                    False                        False  ...   \n",
       "3                                    False                        False  ...   \n",
       "4                                    False                        False  ...   \n",
       "...                                    ...                          ...  ...   \n",
       "18519                                False                        False  ...   \n",
       "18520                                False                        False  ...   \n",
       "18521                                False                        False  ...   \n",
       "18522                                False                        False  ...   \n",
       "18523                                False                        False  ...   \n",
       "\n",
       "       ZINC STAR T-LIGHT HOLDER  ZINC SWEETHEART SOAP DISH  \\\n",
       "0                         False                      False   \n",
       "1                         False                      False   \n",
       "2                         False                      False   \n",
       "3                         False                      False   \n",
       "4                         False                      False   \n",
       "...                         ...                        ...   \n",
       "18519                     False                      False   \n",
       "18520                     False                      False   \n",
       "18521                     False                      False   \n",
       "18522                     False                      False   \n",
       "18523                     False                      False   \n",
       "\n",
       "       ZINC SWEETHEART WIRE LETTER RACK  ZINC T-LIGHT HOLDER STAR LARGE  \\\n",
       "0                                 False                           False   \n",
       "1                                 False                           False   \n",
       "2                                 False                           False   \n",
       "3                                 False                           False   \n",
       "4                                 False                           False   \n",
       "...                                 ...                             ...   \n",
       "18519                             False                           False   \n",
       "18520                             False                           False   \n",
       "18521                             False                            True   \n",
       "18522                             False                           False   \n",
       "18523                             False                           False   \n",
       "\n",
       "       ZINC T-LIGHT HOLDER STARS LARGE  ZINC T-LIGHT HOLDER STARS SMALL  \\\n",
       "0                                False                            False   \n",
       "1                                False                            False   \n",
       "2                                False                            False   \n",
       "3                                False                            False   \n",
       "4                                False                            False   \n",
       "...                                ...                              ...   \n",
       "18519                            False                            False   \n",
       "18520                            False                            False   \n",
       "18521                            False                            False   \n",
       "18522                            False                            False   \n",
       "18523                            False                            False   \n",
       "\n",
       "       ZINC TOP 2 DOOR WOODEN SHELF  ZINC WILLIE WINKIE CANDLE STICK  \\\n",
       "0                             False                            False   \n",
       "1                             False                            False   \n",
       "2                             False                            False   \n",
       "3                             False                            False   \n",
       "4                             False                            False   \n",
       "...                             ...                              ...   \n",
       "18519                         False                            False   \n",
       "18520                         False                            False   \n",
       "18521                         False                             True   \n",
       "18522                         False                            False   \n",
       "18523                         False                            False   \n",
       "\n",
       "       ZINC WIRE KITCHEN ORGANISER  ZINC WIRE SWEETHEART LETTER TRAY  \n",
       "0                            False                             False  \n",
       "1                            False                             False  \n",
       "2                            False                             False  \n",
       "3                            False                             False  \n",
       "4                            False                             False  \n",
       "...                            ...                               ...  \n",
       "18519                        False                             False  \n",
       "18520                        False                             False  \n",
       "18521                        False                             False  \n",
       "18522                        False                             False  \n",
       "18523                        False                             False  \n",
       "\n",
       "[18524 rows x 3858 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "retail_dataset = transaction_df[\"Description\"].to_list()\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(retail_dataset).transform(retail_dataset)\n",
    "transactions = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "transactions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a718591-452e-4d69-be6f-f2e53e2060c7",
   "metadata": {},
   "source": [
    "#### **b(iii)** \n",
    "Use the apriori method on `transactions` to obtain all frequent itemsets using min_support=0.01. Display all frequent itemsets that have at least three items. What support count does an itemset have for our case if it satisfies min_support=0.01?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1463627c-e2c3-43b0-b8c7-f1ad0591f9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.013064</td>\n",
       "      <td>(10 COLOUR SPACEBOY PEN)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010041</td>\n",
       "      <td>(12 MESSAGE CARDS WITH ENVELOPES)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014900</td>\n",
       "      <td>(12 PENCIL SMALL TUBE WOODLAND)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.016573</td>\n",
       "      <td>(12 PENCILS SMALL TUBE RED RETROSPOT)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.015709</td>\n",
       "      <td>(12 PENCILS SMALL TUBE SKULL)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>0.011769</td>\n",
       "      <td>(LUNCH BAG SPACEBOY DESIGN, LUNCH BAG SUKI DES...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>0.014306</td>\n",
       "      <td>(ROSES REGENCY TEACUP AND SAUCER, PINK REGENCY...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>0.010041</td>\n",
       "      <td>(POPPY'S PLAYHOUSE BEDROOM, POPPY'S PLAYHOUSE ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>0.012902</td>\n",
       "      <td>(ROSES REGENCY TEACUP AND SAUCER, GREEN REGENC...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>0.010419</td>\n",
       "      <td>(LUNCH BAG BLACK SKULL., LUNCH BAG PINK POLKAD...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>982 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      support                                           itemsets  length\n",
       "0    0.013064                           (10 COLOUR SPACEBOY PEN)       1\n",
       "1    0.010041                  (12 MESSAGE CARDS WITH ENVELOPES)       1\n",
       "2    0.014900                    (12 PENCIL SMALL TUBE WOODLAND)       1\n",
       "3    0.016573              (12 PENCILS SMALL TUBE RED RETROSPOT)       1\n",
       "4    0.015709                      (12 PENCILS SMALL TUBE SKULL)       1\n",
       "..        ...                                                ...     ...\n",
       "977  0.011769  (LUNCH BAG SPACEBOY DESIGN, LUNCH BAG SUKI DES...       3\n",
       "978  0.014306  (ROSES REGENCY TEACUP AND SAUCER, PINK REGENCY...       3\n",
       "979  0.010041  (POPPY'S PLAYHOUSE BEDROOM, POPPY'S PLAYHOUSE ...       3\n",
       "980  0.012902  (ROSES REGENCY TEACUP AND SAUCER, GREEN REGENC...       4\n",
       "981  0.010419  (LUNCH BAG BLACK SKULL., LUNCH BAG PINK POLKAD...       4\n",
       "\n",
       "[982 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "frequent_itemsets = apriori(transactions, min_support=0.01, use_colnames=True)\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2e4104e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>itemsets</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>0.011607</td>\n",
       "      <td>(ALARM CLOCK BAKELIKE IVORY, ALARM CLOCK BAKEL...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>0.014414</td>\n",
       "      <td>(ALARM CLOCK BAKELIKE PINK, ALARM CLOCK BAKELI...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>0.014630</td>\n",
       "      <td>(GREEN REGENCY TEACUP AND SAUCER, PINK REGENCY...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>0.021054</td>\n",
       "      <td>(ROSES REGENCY TEACUP AND SAUCER, GREEN REGENC...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>0.016843</td>\n",
       "      <td>(ROSES REGENCY TEACUP AND SAUCER, GREEN REGENC...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>0.012524</td>\n",
       "      <td>(JUMBO BAG STRAWBERRY, JUMBO BAG PINK POLKADOT...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>0.010149</td>\n",
       "      <td>(JUMBO SHOPPER VINTAGE RED PAISLEY, JUMBO BAG ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>0.011876</td>\n",
       "      <td>(JUMBO STORAGE BAG SUKI, JUMBO BAG PINK POLKAD...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>0.010095</td>\n",
       "      <td>(JUMBO STORAGE BAG SUKI, JUMBO BAG RED RETROSP...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>0.010095</td>\n",
       "      <td>(LUNCH BAG PINK POLKADOT, LUNCH BAG RED RETROS...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>0.011013</td>\n",
       "      <td>(LUNCH BAG BLACK SKULL., LUNCH BAG RED RETROSP...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>0.010905</td>\n",
       "      <td>(LUNCH BAG BLACK SKULL., LUNCH BAG APPLE DESIG...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>0.010473</td>\n",
       "      <td>(LUNCH BAG PINK POLKADOT, LUNCH BAG RED RETROS...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>936</th>\n",
       "      <td>0.010581</td>\n",
       "      <td>(LUNCH BAG SPACEBOY DESIGN, LUNCH BAG RED RETR...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>0.011769</td>\n",
       "      <td>(LUNCH BAG RED RETROSPOT, LUNCH BAG SUKI DESIG...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>0.010473</td>\n",
       "      <td>(LUNCH BAG RED RETROSPOT, LUNCH BAG WOODLAND, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>0.014036</td>\n",
       "      <td>(LUNCH BAG BLACK SKULL., LUNCH BAG PINK POLKAD...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>0.014144</td>\n",
       "      <td>(LUNCH BAG BLACK SKULL., LUNCH BAG RED RETROSP...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>0.012038</td>\n",
       "      <td>(LUNCH BAG BLACK SKULL., LUNCH BAG SPACEBOY DE...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>0.013604</td>\n",
       "      <td>(LUNCH BAG BLACK SKULL., LUNCH BAG CARS BLUE, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>0.010581</td>\n",
       "      <td>(LUNCH BAG BLACK SKULL., LUNCH BAG CARS BLUE, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>0.016627</td>\n",
       "      <td>(LUNCH BAG BLACK SKULL., LUNCH BAG PINK POLKAD...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>945</th>\n",
       "      <td>0.012146</td>\n",
       "      <td>(LUNCH BAG BLACK SKULL., LUNCH BAG SPACEBOY DE...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>0.013118</td>\n",
       "      <td>(LUNCH BAG BLACK SKULL., LUNCH BAG PINK POLKAD...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>947</th>\n",
       "      <td>0.010689</td>\n",
       "      <td>(LUNCH BAG BLACK SKULL., LUNCH BAG PINK POLKAD...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>0.013874</td>\n",
       "      <td>(LUNCH BAG BLACK SKULL., LUNCH BAG SPACEBOY DE...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>0.015116</td>\n",
       "      <td>(LUNCH BAG BLACK SKULL., LUNCH BAG RED RETROSP...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>0.012416</td>\n",
       "      <td>(LUNCH BAG BLACK SKULL., LUNCH BAG RED RETROSP...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>0.013604</td>\n",
       "      <td>(LUNCH BAG BLACK SKULL., LUNCH BAG SPACEBOY DE...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>952</th>\n",
       "      <td>0.011337</td>\n",
       "      <td>(LUNCH BAG BLACK SKULL., LUNCH BAG SPACEBOY DE...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>0.011283</td>\n",
       "      <td>(LUNCH BAG BLACK SKULL., LUNCH BAG SUKI DESIGN...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>0.015062</td>\n",
       "      <td>(LUNCH BAG PINK POLKADOT, LUNCH BAG RED RETROS...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>0.011984</td>\n",
       "      <td>(LUNCH BAG SPACEBOY DESIGN, LUNCH BAG CARS BLU...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>956</th>\n",
       "      <td>0.012524</td>\n",
       "      <td>(LUNCH BAG PINK POLKADOT, LUNCH BAG CARS BLUE,...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>957</th>\n",
       "      <td>0.010581</td>\n",
       "      <td>(LUNCH BAG PINK POLKADOT, LUNCH BAG CARS BLUE,...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>0.012686</td>\n",
       "      <td>(LUNCH BAG SPACEBOY DESIGN, LUNCH BAG RED RETR...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>959</th>\n",
       "      <td>0.014846</td>\n",
       "      <td>(LUNCH BAG RED RETROSPOT, LUNCH BAG SUKI DESIG...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>0.012416</td>\n",
       "      <td>(LUNCH BAG RED RETROSPOT, LUNCH BAG WOODLAND, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>0.012902</td>\n",
       "      <td>(LUNCH BAG SPACEBOY DESIGN, LUNCH BAG CARS BLU...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>0.011391</td>\n",
       "      <td>(LUNCH BAG SPACEBOY DESIGN, LUNCH BAG CARS BLU...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>0.011445</td>\n",
       "      <td>(LUNCH BAG CARS BLUE, LUNCH BAG SUKI DESIGN, L...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>0.011553</td>\n",
       "      <td>(LUNCH BAG DOLLY GIRL DESIGN, LUNCH BAG SPACEB...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>965</th>\n",
       "      <td>0.010959</td>\n",
       "      <td>(LUNCH BAG DOLLY GIRL DESIGN, LUNCH BAG RED RE...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>0.011067</td>\n",
       "      <td>(LUNCH BAG DOLLY GIRL DESIGN, LUNCH BAG SPACEB...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>0.010095</td>\n",
       "      <td>(LUNCH BAG DOLLY GIRL DESIGN, LUNCH BAG SPACEB...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>968</th>\n",
       "      <td>0.013550</td>\n",
       "      <td>(LUNCH BAG SPACEBOY DESIGN, LUNCH BAG RED RETR...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>969</th>\n",
       "      <td>0.015116</td>\n",
       "      <td>(LUNCH BAG PINK POLKADOT, LUNCH BAG RED RETROS...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>0.012794</td>\n",
       "      <td>(LUNCH BAG PINK POLKADOT, LUNCH BAG RED RETROS...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>0.010689</td>\n",
       "      <td>(LUNCH BAG SPACEBOY DESIGN, LUNCH BAG SUKI DES...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>0.010797</td>\n",
       "      <td>(LUNCH BAG PINK POLKADOT, LUNCH BAG SPACEBOY D...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>0.010581</td>\n",
       "      <td>(LUNCH BAG PINK POLKADOT, LUNCH BAG SUKI DESIG...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>0.014630</td>\n",
       "      <td>(LUNCH BAG SPACEBOY DESIGN, LUNCH BAG RED RETR...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>0.013928</td>\n",
       "      <td>(LUNCH BAG SPACEBOY DESIGN, LUNCH BAG RED RETR...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>0.013766</td>\n",
       "      <td>(LUNCH BAG RED RETROSPOT, LUNCH BAG SUKI DESIG...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>0.011769</td>\n",
       "      <td>(LUNCH BAG SPACEBOY DESIGN, LUNCH BAG SUKI DES...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>0.014306</td>\n",
       "      <td>(ROSES REGENCY TEACUP AND SAUCER, PINK REGENCY...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>0.010041</td>\n",
       "      <td>(POPPY'S PLAYHOUSE BEDROOM, POPPY'S PLAYHOUSE ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      support                                           itemsets  length\n",
       "923  0.011607  (ALARM CLOCK BAKELIKE IVORY, ALARM CLOCK BAKEL...       3\n",
       "924  0.014414  (ALARM CLOCK BAKELIKE PINK, ALARM CLOCK BAKELI...       3\n",
       "925  0.014630  (GREEN REGENCY TEACUP AND SAUCER, PINK REGENCY...       3\n",
       "926  0.021054  (ROSES REGENCY TEACUP AND SAUCER, GREEN REGENC...       3\n",
       "927  0.016843  (ROSES REGENCY TEACUP AND SAUCER, GREEN REGENC...       3\n",
       "928  0.012524  (JUMBO BAG STRAWBERRY, JUMBO BAG PINK POLKADOT...       3\n",
       "929  0.010149  (JUMBO SHOPPER VINTAGE RED PAISLEY, JUMBO BAG ...       3\n",
       "930  0.011876  (JUMBO STORAGE BAG SUKI, JUMBO BAG PINK POLKAD...       3\n",
       "931  0.010095  (JUMBO STORAGE BAG SUKI, JUMBO BAG RED RETROSP...       3\n",
       "932  0.010095  (LUNCH BAG PINK POLKADOT, LUNCH BAG RED RETROS...       3\n",
       "933  0.011013  (LUNCH BAG BLACK SKULL., LUNCH BAG RED RETROSP...       3\n",
       "934  0.010905  (LUNCH BAG BLACK SKULL., LUNCH BAG APPLE DESIG...       3\n",
       "935  0.010473  (LUNCH BAG PINK POLKADOT, LUNCH BAG RED RETROS...       3\n",
       "936  0.010581  (LUNCH BAG SPACEBOY DESIGN, LUNCH BAG RED RETR...       3\n",
       "937  0.011769  (LUNCH BAG RED RETROSPOT, LUNCH BAG SUKI DESIG...       3\n",
       "938  0.010473  (LUNCH BAG RED RETROSPOT, LUNCH BAG WOODLAND, ...       3\n",
       "939  0.014036  (LUNCH BAG BLACK SKULL., LUNCH BAG PINK POLKAD...       3\n",
       "940  0.014144  (LUNCH BAG BLACK SKULL., LUNCH BAG RED RETROSP...       3\n",
       "941  0.012038  (LUNCH BAG BLACK SKULL., LUNCH BAG SPACEBOY DE...       3\n",
       "942  0.013604  (LUNCH BAG BLACK SKULL., LUNCH BAG CARS BLUE, ...       3\n",
       "943  0.010581  (LUNCH BAG BLACK SKULL., LUNCH BAG CARS BLUE, ...       3\n",
       "944  0.016627  (LUNCH BAG BLACK SKULL., LUNCH BAG PINK POLKAD...       3\n",
       "945  0.012146  (LUNCH BAG BLACK SKULL., LUNCH BAG SPACEBOY DE...       3\n",
       "946  0.013118  (LUNCH BAG BLACK SKULL., LUNCH BAG PINK POLKAD...       3\n",
       "947  0.010689  (LUNCH BAG BLACK SKULL., LUNCH BAG PINK POLKAD...       3\n",
       "948  0.013874  (LUNCH BAG BLACK SKULL., LUNCH BAG SPACEBOY DE...       3\n",
       "949  0.015116  (LUNCH BAG BLACK SKULL., LUNCH BAG RED RETROSP...       3\n",
       "950  0.012416  (LUNCH BAG BLACK SKULL., LUNCH BAG RED RETROSP...       3\n",
       "951  0.013604  (LUNCH BAG BLACK SKULL., LUNCH BAG SPACEBOY DE...       3\n",
       "952  0.011337  (LUNCH BAG BLACK SKULL., LUNCH BAG SPACEBOY DE...       3\n",
       "953  0.011283  (LUNCH BAG BLACK SKULL., LUNCH BAG SUKI DESIGN...       3\n",
       "954  0.015062  (LUNCH BAG PINK POLKADOT, LUNCH BAG RED RETROS...       3\n",
       "955  0.011984  (LUNCH BAG SPACEBOY DESIGN, LUNCH BAG CARS BLU...       3\n",
       "956  0.012524  (LUNCH BAG PINK POLKADOT, LUNCH BAG CARS BLUE,...       3\n",
       "957  0.010581  (LUNCH BAG PINK POLKADOT, LUNCH BAG CARS BLUE,...       3\n",
       "958  0.012686  (LUNCH BAG SPACEBOY DESIGN, LUNCH BAG RED RETR...       3\n",
       "959  0.014846  (LUNCH BAG RED RETROSPOT, LUNCH BAG SUKI DESIG...       3\n",
       "960  0.012416  (LUNCH BAG RED RETROSPOT, LUNCH BAG WOODLAND, ...       3\n",
       "961  0.012902  (LUNCH BAG SPACEBOY DESIGN, LUNCH BAG CARS BLU...       3\n",
       "962  0.011391  (LUNCH BAG SPACEBOY DESIGN, LUNCH BAG CARS BLU...       3\n",
       "963  0.011445  (LUNCH BAG CARS BLUE, LUNCH BAG SUKI DESIGN, L...       3\n",
       "964  0.011553  (LUNCH BAG DOLLY GIRL DESIGN, LUNCH BAG SPACEB...       3\n",
       "965  0.010959  (LUNCH BAG DOLLY GIRL DESIGN, LUNCH BAG RED RE...       3\n",
       "966  0.011067  (LUNCH BAG DOLLY GIRL DESIGN, LUNCH BAG SPACEB...       3\n",
       "967  0.010095  (LUNCH BAG DOLLY GIRL DESIGN, LUNCH BAG SPACEB...       3\n",
       "968  0.013550  (LUNCH BAG SPACEBOY DESIGN, LUNCH BAG RED RETR...       3\n",
       "969  0.015116  (LUNCH BAG PINK POLKADOT, LUNCH BAG RED RETROS...       3\n",
       "970  0.012794  (LUNCH BAG PINK POLKADOT, LUNCH BAG RED RETROS...       3\n",
       "971  0.010689  (LUNCH BAG SPACEBOY DESIGN, LUNCH BAG SUKI DES...       3\n",
       "972  0.010797  (LUNCH BAG PINK POLKADOT, LUNCH BAG SPACEBOY D...       3\n",
       "973  0.010581  (LUNCH BAG PINK POLKADOT, LUNCH BAG SUKI DESIG...       3\n",
       "974  0.014630  (LUNCH BAG SPACEBOY DESIGN, LUNCH BAG RED RETR...       3\n",
       "975  0.013928  (LUNCH BAG SPACEBOY DESIGN, LUNCH BAG RED RETR...       3\n",
       "976  0.013766  (LUNCH BAG RED RETROSPOT, LUNCH BAG SUKI DESIG...       3\n",
       "977  0.011769  (LUNCH BAG SPACEBOY DESIGN, LUNCH BAG SUKI DES...       3\n",
       "978  0.014306  (ROSES REGENCY TEACUP AND SAUCER, PINK REGENCY...       3\n",
       "979  0.010041  (POPPY'S PLAYHOUSE BEDROOM, POPPY'S PLAYHOUSE ...       3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequent_itemsets[(frequent_itemsets['length'] == 3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ad89a8-70d2-4734-9780-c964e14fbf23",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb37dd-da50-4b68-988d-23500611955b",
   "metadata": {},
   "source": [
    "#### **b(iv)**\n",
    "Now we will discover association rules from the frequent itemsets. Using only the frequent itemsets with min_support=0.01 (the ones obtained in **b(iii)**), generate different association rules using min_conf=0.6 and min_conf=0.9 as thresholds. Show the association rules for each of the thresholds. What do you notice w.r.t. the number of association rules produced for the different thresholds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aba91aa5-f2ef-4e68-a70c-6f112c3b9ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(ALARM CLOCK BAKELIKE CHOCOLATE)</td>\n",
       "      <td>(ALARM CLOCK BAKELIKE GREEN)</td>\n",
       "      <td>0.017383</td>\n",
       "      <td>0.042593</td>\n",
       "      <td>0.011337</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>15.311622</td>\n",
       "      <td>0.010596</td>\n",
       "      <td>2.752544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(ALARM CLOCK BAKELIKE CHOCOLATE)</td>\n",
       "      <td>(ALARM CLOCK BAKELIKE RED)</td>\n",
       "      <td>0.017383</td>\n",
       "      <td>0.047290</td>\n",
       "      <td>0.012200</td>\n",
       "      <td>0.701863</td>\n",
       "      <td>14.841686</td>\n",
       "      <td>0.011378</td>\n",
       "      <td>3.195548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(ALARM CLOCK BAKELIKE ORANGE)</td>\n",
       "      <td>(ALARM CLOCK BAKELIKE GREEN)</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.042593</td>\n",
       "      <td>0.011876</td>\n",
       "      <td>0.612813</td>\n",
       "      <td>14.387522</td>\n",
       "      <td>0.011051</td>\n",
       "      <td>2.472726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(ALARM CLOCK BAKELIKE RED)</td>\n",
       "      <td>(ALARM CLOCK BAKELIKE GREEN)</td>\n",
       "      <td>0.047290</td>\n",
       "      <td>0.042593</td>\n",
       "      <td>0.028612</td>\n",
       "      <td>0.605023</td>\n",
       "      <td>14.204617</td>\n",
       "      <td>0.026597</td>\n",
       "      <td>2.423954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(ALARM CLOCK BAKELIKE GREEN)</td>\n",
       "      <td>(ALARM CLOCK BAKELIKE RED)</td>\n",
       "      <td>0.042593</td>\n",
       "      <td>0.047290</td>\n",
       "      <td>0.028612</td>\n",
       "      <td>0.671736</td>\n",
       "      <td>14.204617</td>\n",
       "      <td>0.026597</td>\n",
       "      <td>2.902271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>(PINK REGENCY TEACUP AND SAUCER, REGENCY CAKES...</td>\n",
       "      <td>(ROSES REGENCY TEACUP AND SAUCER, GREEN REGENC...</td>\n",
       "      <td>0.016681</td>\n",
       "      <td>0.029205</td>\n",
       "      <td>0.012902</td>\n",
       "      <td>0.773463</td>\n",
       "      <td>26.483594</td>\n",
       "      <td>0.012415</td>\n",
       "      <td>4.285365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>(LUNCH BAG BLACK SKULL., LUNCH BAG PINK POLKAD...</td>\n",
       "      <td>(LUNCH BAG CARS BLUE)</td>\n",
       "      <td>0.016627</td>\n",
       "      <td>0.052149</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.626623</td>\n",
       "      <td>12.016119</td>\n",
       "      <td>0.009552</td>\n",
       "      <td>2.538593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>(LUNCH BAG BLACK SKULL., LUNCH BAG PINK POLKAD...</td>\n",
       "      <td>(LUNCH BAG RED RETROSPOT)</td>\n",
       "      <td>0.014036</td>\n",
       "      <td>0.069531</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.742308</td>\n",
       "      <td>10.675860</td>\n",
       "      <td>0.009443</td>\n",
       "      <td>3.610774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>(LUNCH BAG BLACK SKULL., LUNCH BAG RED RETROSP...</td>\n",
       "      <td>(LUNCH BAG PINK POLKADOT)</td>\n",
       "      <td>0.014144</td>\n",
       "      <td>0.050259</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.736641</td>\n",
       "      <td>14.656866</td>\n",
       "      <td>0.009708</td>\n",
       "      <td>3.606262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>(LUNCH BAG PINK POLKADOT, LUNCH BAG RED RETROS...</td>\n",
       "      <td>(LUNCH BAG BLACK SKULL.)</td>\n",
       "      <td>0.015062</td>\n",
       "      <td>0.056791</td>\n",
       "      <td>0.010419</td>\n",
       "      <td>0.691756</td>\n",
       "      <td>12.180697</td>\n",
       "      <td>0.009564</td>\n",
       "      <td>3.059945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           antecedents  \\\n",
       "0                     (ALARM CLOCK BAKELIKE CHOCOLATE)   \n",
       "1                     (ALARM CLOCK BAKELIKE CHOCOLATE)   \n",
       "2                        (ALARM CLOCK BAKELIKE ORANGE)   \n",
       "3                           (ALARM CLOCK BAKELIKE RED)   \n",
       "4                         (ALARM CLOCK BAKELIKE GREEN)   \n",
       "..                                                 ...   \n",
       "145  (PINK REGENCY TEACUP AND SAUCER, REGENCY CAKES...   \n",
       "146  (LUNCH BAG BLACK SKULL., LUNCH BAG PINK POLKAD...   \n",
       "147  (LUNCH BAG BLACK SKULL., LUNCH BAG PINK POLKAD...   \n",
       "148  (LUNCH BAG BLACK SKULL., LUNCH BAG RED RETROSP...   \n",
       "149  (LUNCH BAG PINK POLKADOT, LUNCH BAG RED RETROS...   \n",
       "\n",
       "                                           consequents  antecedent support  \\\n",
       "0                         (ALARM CLOCK BAKELIKE GREEN)            0.017383   \n",
       "1                           (ALARM CLOCK BAKELIKE RED)            0.017383   \n",
       "2                         (ALARM CLOCK BAKELIKE GREEN)            0.019380   \n",
       "3                         (ALARM CLOCK BAKELIKE GREEN)            0.047290   \n",
       "4                           (ALARM CLOCK BAKELIKE RED)            0.042593   \n",
       "..                                                 ...                 ...   \n",
       "145  (ROSES REGENCY TEACUP AND SAUCER, GREEN REGENC...            0.016681   \n",
       "146                              (LUNCH BAG CARS BLUE)            0.016627   \n",
       "147                          (LUNCH BAG RED RETROSPOT)            0.014036   \n",
       "148                          (LUNCH BAG PINK POLKADOT)            0.014144   \n",
       "149                           (LUNCH BAG BLACK SKULL.)            0.015062   \n",
       "\n",
       "     consequent support   support  confidence       lift  leverage  conviction  \n",
       "0              0.042593  0.011337    0.652174  15.311622  0.010596    2.752544  \n",
       "1              0.047290  0.012200    0.701863  14.841686  0.011378    3.195548  \n",
       "2              0.042593  0.011876    0.612813  14.387522  0.011051    2.472726  \n",
       "3              0.042593  0.028612    0.605023  14.204617  0.026597    2.423954  \n",
       "4              0.047290  0.028612    0.671736  14.204617  0.026597    2.902271  \n",
       "..                  ...       ...         ...        ...       ...         ...  \n",
       "145            0.029205  0.012902    0.773463  26.483594  0.012415    4.285365  \n",
       "146            0.052149  0.010419    0.626623  12.016119  0.009552    2.538593  \n",
       "147            0.069531  0.010419    0.742308  10.675860  0.009443    3.610774  \n",
       "148            0.050259  0.010419    0.736641  14.656866  0.009708    3.606262  \n",
       "149            0.056791  0.010419    0.691756  12.180697  0.009564    3.059945  \n",
       "\n",
       "[150 rows x 9 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "rules_6 = arule(frequent_itemsets[['support', 'itemsets']], metric=\"confidence\", min_threshold=0.6)\n",
    "rules_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ea0e025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(REGENCY TEA PLATE PINK)</td>\n",
       "      <td>(REGENCY TEA PLATE GREEN)</td>\n",
       "      <td>0.012092</td>\n",
       "      <td>0.014576</td>\n",
       "      <td>0.010905</td>\n",
       "      <td>0.901786</td>\n",
       "      <td>61.869180</td>\n",
       "      <td>0.010729</td>\n",
       "      <td>10.033411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(POPPY'S PLAYHOUSE BEDROOM, POPPY'S PLAYHOUSE ...</td>\n",
       "      <td>(POPPY'S PLAYHOUSE KITCHEN)</td>\n",
       "      <td>0.011067</td>\n",
       "      <td>0.018678</td>\n",
       "      <td>0.010041</td>\n",
       "      <td>0.907317</td>\n",
       "      <td>48.575553</td>\n",
       "      <td>0.009834</td>\n",
       "      <td>10.587943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(ROSES REGENCY TEACUP AND SAUCER, PINK REGENCY...</td>\n",
       "      <td>(GREEN REGENCY TEACUP AND SAUCER)</td>\n",
       "      <td>0.014306</td>\n",
       "      <td>0.037303</td>\n",
       "      <td>0.012902</td>\n",
       "      <td>0.901887</td>\n",
       "      <td>24.177353</td>\n",
       "      <td>0.012369</td>\n",
       "      <td>9.812104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         antecedents  \\\n",
       "0                           (REGENCY TEA PLATE PINK)   \n",
       "1  (POPPY'S PLAYHOUSE BEDROOM, POPPY'S PLAYHOUSE ...   \n",
       "2  (ROSES REGENCY TEACUP AND SAUCER, PINK REGENCY...   \n",
       "\n",
       "                         consequents  antecedent support  consequent support  \\\n",
       "0          (REGENCY TEA PLATE GREEN)            0.012092            0.014576   \n",
       "1        (POPPY'S PLAYHOUSE KITCHEN)            0.011067            0.018678   \n",
       "2  (GREEN REGENCY TEACUP AND SAUCER)            0.014306            0.037303   \n",
       "\n",
       "    support  confidence       lift  leverage  conviction  \n",
       "0  0.010905    0.901786  61.869180  0.010729   10.033411  \n",
       "1  0.010041    0.907317  48.575553  0.009834   10.587943  \n",
       "2  0.012902    0.901887  24.177353  0.012369    9.812104  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rules_9 = arule(frequent_itemsets[['support', 'itemsets']], metric=\"confidence\", min_threshold=0.9)\n",
    "rules_9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fca001-2e05-453a-970a-0155a1c83fbd",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1920ddaf-f961-4c50-b34b-d5d338223675",
   "metadata": {},
   "source": [
    "#### **b(v)** \n",
    "From the association rules that satisfy the confidence threshold 0.6, select and show the two rules with the highest lift. What do you notice if you compare the two rules with each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4110f530-8c31-4be4-96f2-aaa5d4595130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>antecedents</th>\n",
       "      <th>consequents</th>\n",
       "      <th>antecedent support</th>\n",
       "      <th>consequent support</th>\n",
       "      <th>support</th>\n",
       "      <th>confidence</th>\n",
       "      <th>lift</th>\n",
       "      <th>leverage</th>\n",
       "      <th>conviction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>(REGENCY TEA PLATE GREEN)</td>\n",
       "      <td>(REGENCY TEA PLATE PINK)</td>\n",
       "      <td>0.014576</td>\n",
       "      <td>0.012092</td>\n",
       "      <td>0.010905</td>\n",
       "      <td>0.748148</td>\n",
       "      <td>61.86918</td>\n",
       "      <td>0.010729</td>\n",
       "      <td>3.922574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>(REGENCY TEA PLATE PINK)</td>\n",
       "      <td>(REGENCY TEA PLATE GREEN)</td>\n",
       "      <td>0.012092</td>\n",
       "      <td>0.014576</td>\n",
       "      <td>0.010905</td>\n",
       "      <td>0.901786</td>\n",
       "      <td>61.86918</td>\n",
       "      <td>0.010729</td>\n",
       "      <td>10.033411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  antecedents                consequents  antecedent support  \\\n",
       "53  (REGENCY TEA PLATE GREEN)   (REGENCY TEA PLATE PINK)            0.014576   \n",
       "54   (REGENCY TEA PLATE PINK)  (REGENCY TEA PLATE GREEN)            0.012092   \n",
       "\n",
       "    consequent support   support  confidence      lift  leverage  conviction  \n",
       "53            0.012092  0.010905    0.748148  61.86918  0.010729    3.922574  \n",
       "54            0.014576  0.010905    0.901786  61.86918  0.010729   10.033411  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "rules_6.sort_values('lift',ascending = False).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743f10f9-2067-4d68-9510-33a5699e4ae8",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc2b990-3a2d-409c-907a-de7a812aa687",
   "metadata": {},
   "source": [
    "#### **b(vi)** \n",
    "\n",
    "In the analysis tasks in **b)**, an itemset consisted of items that had the same invoice number (same transaction items). Thus, if an itemset was frequent, it meant that the items in it were frequently purchased together.\n",
    "An association rule $A \\Rightarrow B$ meant that if items in $A$ are purchased, then the items in $B$ are also purchased in that same transaction.\n",
    "\n",
    "Suppose that we would repeat the analysis, but this time, the itemsets would consist of items having the same \"CustomerID\" (bought from the same customer). Interpret the meaning of the frequent itemsets and association rules for this kind of itemsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7269908-782f-4900-a306-7507b7afd115",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a438848-5297-4ca8-9e09-cf414903197b",
   "metadata": {},
   "source": [
    "### c) Sequence Mining (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b2c05a-2fa5-4150-b5ce-895d9f1078cc",
   "metadata": {},
   "source": [
    "For this task, the dataset used is `retail_sequences.csv`. Run the cell below to save the dataset under the dataframe `retail_sequences`. Each row in the dataframe corresponds to a unique customer (from the retail dataset). The \"Customer\" column contains the customer ID, whereas the \"Sequence\" column contains the sequence of itemsets  purchased by that customer.  Each value of \"Sequence\" is a sequence (list) of itemsets $<I_1, I_2, ..., I_n>$. The items within the same itemset (list without duplicates) $I_i$ were purchased together (they had the same invoice number). The itemsets are ordered by the timestamp of the transaction (value of InvoiceDate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5764854-1c5e-44a8-b83a-c1bed469e9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer</th>\n",
       "      <th>Sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>178500</td>\n",
       "      <td>[['white hanging heart t-light holder', 'white...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>130470</td>\n",
       "      <td>[['assorted colour bird ornament', \"poppy's pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>125830</td>\n",
       "      <td>[['alarm clock bakelike pink', 'alarm clock ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137480</td>\n",
       "      <td>[[\"paper chain kit 50's christmas\"], ['biscuit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>151000</td>\n",
       "      <td>[['victorian sewing box large'], ['victorian s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4329</th>\n",
       "      <td>134360</td>\n",
       "      <td>[['victorian glass hanging t-light', 'regency ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4330</th>\n",
       "      <td>155200</td>\n",
       "      <td>[['fridge magnets us diner assorted', 'fridge ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4331</th>\n",
       "      <td>132980</td>\n",
       "      <td>[['knitted union flag hot water bottle', 'whit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4332</th>\n",
       "      <td>145690</td>\n",
       "      <td>[['fairy tale cottage night light', 'red toads...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4333</th>\n",
       "      <td>127130</td>\n",
       "      <td>[['postage', 'set/10 blue polkadot party candl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4334 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Customer                                           Sequence\n",
       "0       178500  [['white hanging heart t-light holder', 'white...\n",
       "1       130470  [['assorted colour bird ornament', \"poppy's pl...\n",
       "2       125830  [['alarm clock bakelike pink', 'alarm clock ba...\n",
       "3       137480  [[\"paper chain kit 50's christmas\"], ['biscuit...\n",
       "4       151000  [['victorian sewing box large'], ['victorian s...\n",
       "...        ...                                                ...\n",
       "4329    134360  [['victorian glass hanging t-light', 'regency ...\n",
       "4330    155200  [['fridge magnets us diner assorted', 'fridge ...\n",
       "4331    132980  [['knitted union flag hot water bottle', 'whit...\n",
       "4332    145690  [['fairy tale cottage night light', 'red toads...\n",
       "4333    127130  [['postage', 'set/10 blue polkadot party candl...\n",
       "\n",
       "[4334 rows x 2 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "retail_sequences = pd.read_csv('datasets/retail_sequences.csv')\n",
    "retail_sequences\n",
    "#retail_sequences = pd.read_csv('datasets/retail_sequences.csv', converters={'Sequence': pd.eval})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6769f289-6619-41f4-855d-d5a690009e7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Given is the sequence *s= <{'lunch bag cars blue'}, {'herb marker rosemary','herb marker thyme'}, {'wooden star christmas scandinavian'}>*. Compute the support count of that sequence, that is, compute the number of customers whose corresponding itemset sequence contains it. Display its support count and the IDs of those customers.\n",
    "\n",
    "*Hint: In the dataset provided, all product names are unified. They are all lowercase and have no trailing spaces.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff9fcd7-4dc8-48a9-8ae0-5c8f0e0d0f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = [['lunch bag cars blue'], ['herb marker rosemary', 'herb marker thyme'] ,['wooden star christmas scandinavian']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006414b7-f6eb-4ea6-8d76-9ac57408501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb34ba2-6c45-4dc7-97f1-0194de53b3f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 4: Text Mining (12 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4c2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c87d2b-a192-496d-ae0e-52865d498d72",
   "metadata": {},
   "source": [
    "### F.R.I.E.N.D.S."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725586d9-d73b-40bd-938d-e62dcae05fee",
   "metadata": {},
   "source": [
    "In this task we will use the script from the well-known series \"F.R.I.E.N.D.S.\". We will apply feature extraction methods to map the line of each main character onto a vector of a vector space. Then we will train a classifier whose aim will be to predict the name of the character given a particular line from the script.\n",
    "In the end, we will train language models using N-grams and produce fake sentences for each of the main characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945cdb03-6f30-4b39-93b5-11ae64b60c4e",
   "metadata": {},
   "source": [
    "### a) Data Loading and Preprocessing (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ce8b2-8381-4447-8c70-8a1f66fa4b17",
   "metadata": {},
   "source": [
    "#### **a(i)** \n",
    "Import the file `FRIENDS.csv` and save it into a dataframe named `friends_df`. Note that the dataframe must contain two columns: one indicating the character's name and one containing a line from the script. Display the first few lines from the dataframe.\n",
    "\n",
    "<i>FYI: The script has been filtered so that it only contains lines from the main characters. The order of the lines in the data is the same as the order of the lines in the original script. Metadata and scene descriptions have been removed. Your corpus consists of all the lines contained in the data. Each row's \"line\" value is a single document. </i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d249dbeb-b7c9-48f3-8fb8-546d052daac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8ecf98-6140-4c73-9350-08060d26deee",
   "metadata": {},
   "source": [
    "#### **a(ii)**  \n",
    "Plot the line count distribution among the six main characters (the six possible values of the column \"character\"). For example, show a plot containing one bar for each character whose height reflects the number of lines in `friends_df`. Briefly comment on the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534f1199-6f25-4e00-83c7-8aefbbe75250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe800c4f-7a8b-4c45-9484-ffeedb161a2e",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c20766-da32-4e64-99af-1f8eb1a466e1",
   "metadata": {},
   "source": [
    "#### **a(iii)**  \n",
    "Create a corpus named `corpus` such that each document in the corpus corresponds to exactly one row's \"line\" in `friends_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a677e05f-51c6-431d-933f-961089327685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a5e192-46e1-4c64-92d2-387c83e8d4c2",
   "metadata": {},
   "source": [
    "#### **a(iv)**  \n",
    "Write a function called `my_preprocessor` which, given a string, returns another string after tokenization, stopword removal and lemmatization have been applied. The remaining terms (tokens after stopword removal and lemmatization has been applied) should be joined in the same string using space ' '."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5365fc69-a32d-4b8a-a142-0ec4ae8aa988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b173fb1e-44ef-4e72-a087-2cc67d5464b1",
   "metadata": {},
   "source": [
    "#### **a(v)**   \n",
    "You must apply your preprocessor `my_preprocessor` on each line contained in the `corpus`. Create a preprocessed corpus named `corpus_p` which contains the same lines as `corpus` after the preprocessor `my_preprocessor` has been applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46ad877-6f7d-45af-902a-313efaf6e9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f6a33-d791-493d-b9a3-809cfad2d694",
   "metadata": {},
   "source": [
    "#### **a(vi)**   \n",
    "Split the `friends_df` dataset from the previous task into training (80%) and test (20%) data preserving the distribution based on the \"character\" value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae634d-d8d3-4057-84a8-aa4117cf73dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d998c5-75fc-4673-bb69-2a4a644b8681",
   "metadata": {},
   "source": [
    "#### **a(vii)**   \n",
    "Similar to **a(iii)**, for the training data and the test data, create two corpora named `corpus_train` and `corpus_test` respectively. Each document in the training (test) corpus must correspond to exactly one row's \"line\" value in the corresponding training (test) dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8fa13d-599d-4c60-b747-bde77c9e99d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8945a0-a108-4380-b0fa-aec46768c284",
   "metadata": {},
   "source": [
    "### b) Set of Words (4.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a862af1a-83a6-46fd-ab0e-48a55d2667e9",
   "metadata": {},
   "source": [
    "#### **b(i)**  \n",
    "We want to encode our text in such a way that for each word in the vocabulary, we are only interested in whether the word appears or not in a given document. Create such a Set of Words encoding for the whole corpus `corpus`. Use the previously defined preprocessor `my_preprocessor` as preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000b068a-d85d-405a-a6b2-b463d63afa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e607c40e-1b8a-4bda-9dc2-0cc4f5872452",
   "metadata": {},
   "source": [
    "#### **b(ii)**  \n",
    "Pick one (any) of the lines of the \"line\" column in the `friends_df` dataset. Display the line in:\n",
    "    1) its original form, \n",
    "    2) its preprocessed version (the result contained in `corpus_p` after applying `my_preprocessor`), and \n",
    "    3) its encoding computed by the Set of Words method. This can be either an array (a vector) or a scipy matrix. \n",
    "Briefly comment on the Set of Words encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152e71ca-9019-47b1-a827-b7227e85a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b431ce-c9ef-46fd-80f9-5c0ee685770b",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c704be66-78e8-40b5-9f88-3df1939000e6",
   "metadata": {},
   "source": [
    "#### **b(iii)**  \n",
    "Create a Set of Words encoding based only on the documents in `corpus_train`. Use the previously defined preprocessor `my_preprocessor` as a preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbedb7eb-192a-4f0d-b590-9cf3cf8a813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26cad99-701b-4e56-a020-5e2d1ef04a4d",
   "metadata": {},
   "source": [
    "#### **b(iv)**  \n",
    "In this task, we will use an SGD (Stochastic Gradient Descend) classifier to predict the character given a line from the corpus. Train the classifier on the Set of Words encoding of training corpus `corpus_train` using the character as the target feature and 'log_loss' as the loss function. Apply the classifier on the Set of Words encodings of both the training corpus and the test corpus `corpus_train`. Show its accuracy for both the training corpus and the test corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeffa1d-bb0b-4112-846c-74cde3e217dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77974b1-4fb7-4fa9-9ef5-40764d4a0a28",
   "metadata": {},
   "source": [
    "#### **b(v)**  \n",
    "Briefly comment on the accuracy of the classifier compared to the expected accuracy of a random guesser (here: a model that simply guesses each character according to a distribution based on the line count). Use the line count distribution shown in **a(ii)** to reason about the approximate accuracy of the random guesser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7d8d1-3375-49b1-b69e-a4e2da8c184e",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe69290-07c2-4f7f-8693-d05884b98963",
   "metadata": {},
   "source": [
    "#### **b(vi)**  \n",
    "Pick two lines from the dataset `friends_df`. Predict their character by applying the SGD classifier from **b(iv)** to their Set of Words encodings. Show the original lines, their original characters and the predicted characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7621f1-8fc9-4606-9c19-0fb007e13b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccc83fe-08a7-4109-a68d-765799874a83",
   "metadata": {},
   "source": [
    "### c) Doc2Vec (1.5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cca937f-cc73-496b-90df-3d571b4ebd3b",
   "metadata": {},
   "source": [
    "#### **c(i)**  \n",
    "In this part, we want to encode the lines using Doc2Vec. Create a Doc2Vec embedding based on the documents in the preprocessed corpus `corpus_p`. Set the vector dimension to 10 and min_count to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd41ee-d39a-43a3-a7c0-eb2e6f1c9a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee6650f-1ca5-47aa-adfd-56c33c6dc4fd",
   "metadata": {},
   "source": [
    "#### **c(ii)**  \n",
    "Pick one (any) line from the dataset `friends_df`. Display the line and the character saying it. Find its most similar line w.r.t. the Doc2Vec encoding and display the original line and its corresponding character. Do the lines belong to the same character?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f543244-67a5-4d00-aff1-9a5a05518ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437b009f-0d60-4f3c-857a-0ff2c62763af",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae3f695-65a1-4477-8909-517ec5133e5c",
   "metadata": {},
   "source": [
    "### d) Language model using N-grams (2 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2d9dad-ffc3-4ae0-8c31-7ac550656871",
   "metadata": {},
   "source": [
    "#### For the following tasks, use the `friends_df` data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da82a84-2f4b-44e6-9138-deab47dbeb92",
   "metadata": {},
   "source": [
    "#### **d(i)**  \n",
    "For each character, create a corresponding corpus. Each corpus must be a list of documents. Each document corresponds to one \"line\" value of that character and it should be a list of terms. You must obtain this list of terms after applying preprocessing steps such as to lowercase, no punctuation, and tokenization to the original line. Do not perform stemming/lemmatization and/or stopword removal for this task.\n",
    "Display the corpus of one of the characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de295c77-0ef1-423e-ac2d-2dd4241d22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e162de2-271d-4031-858b-40f5dcbc5506",
   "metadata": {},
   "source": [
    "#### **d(ii)**  \n",
    "For each character separately, build a trigram language model using MLE. Use both right and left padding and learn each language model using the character's corpus from **d(ii)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838f330b-20b3-4c1a-af32-6a84d45330fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e734c4b-2c31-4f19-9f43-4cf2983d8076",
   "metadata": {},
   "source": [
    "#### **d(iv)**  \n",
    "For each character, use the created trigram language model to generate a sentence of ten words. Display the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418add81-ad45-4712-905d-4ae71010f251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788da653-9059-4045-9341-da119995bc4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 5: Process Mining (22pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75571aa7-6567-44de-86e3-27aba8d5d03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pm4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470d33e5-5fea-4f90-baba-dfc4c7fc72bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.statistics.traces.generic.log import case_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4472e969-94fb-47e1-bb32-6dcfd7f154b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.algo.conformance.tokenreplay import algorithm as token_based_replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da87cd3-a2f0-46ee-97e2-c3c3c9905140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fd03f7-1c78-45be-9fb1-018779e47fd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### a) Loading the Data and Basic Statistics (9pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e696437-3c4a-4e5f-9725-b12d153d1688",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **a(i)** \n",
    "Load the data **reimburse.csv** and create a PM4Py event log. In doing so, use the following column mapping:\n",
    " - *Activity* is the activity key\n",
    " - *Case* is the case ID\n",
    " - *Timestamp* is the timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d14d031-f77b-4b7c-807a-b352e2e4a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cdcace-82db-451c-8d8a-2fe3d7033df2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **a(ii)** \n",
    "Compute and print the following basic information:\n",
    "- Number of events\n",
    "- Number of cases\n",
    "- Earliest timestamp\n",
    "- Latest timestamp\n",
    "- Number of trace variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d1b421-7308-4031-a5b2-bbf4cbfa18e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a45f421-ded8-4ebc-bfcd-bde5d3e3069b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **a(iii)** \n",
    "In process mining, multiple events that have the same timestamp can cause problems because the ordering of events (or even activities) becomes unclear. Moreover, they can indicate batching (i.e., one activity is executed for multiple cases simultaneously). Therefore, during your analysis, it is good to keep that in mind. To this end, compute the following statistics/answer the following questions:\n",
    "\n",
    "1. How many events occur almost at the same time (i.e., within less than 100ms as the preceding event. (Proceeding event in the *entire* event log)?\n",
    "2. Are there resources that complete two activities at the same time (within less than 100ms)?\n",
    "3. How many cases are there in which two activities are executed at the same time (i.e., two events that belong to the same case occur within less than 100ms)?\n",
    "\n",
    "*Hint: Depending on how you find the answers, be careful about event orderings.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70064816-159a-4092-92ec-d7d59e21c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75d70c8-9b9a-4444-bd50-3e6d78c25bc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **a(iv)** \n",
    "Provide a plot that shows the number of running cases (i.e., cases that have started but not yet finished) over time. You may assume that the log only contains complete traces. For each case that has started, its completion is the last observed event associated with that case. In case multiple cases start or end at the same time, you also generate multiple values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522aaab1-e228-430a-bafd-3d1e82101aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9e3d73-265e-4161-8bfe-48a16643d42c",
   "metadata": {},
   "source": [
    "**Your answer**: *(Briefly describe the differences between the two models in about five sentences here.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71e62b5-cc4e-4f3c-b836-08455477b93f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### b) Discovery and Conformance Checking (9pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3d450-49b1-479f-9e99-9760f8b9bd14",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(i)** \n",
    "Mine a Petri net using Inductive Miner and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6417094f-6d92-4f1c-b63c-57588f16be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32b33dd-e8f5-46a7-be13-4821fbbf9e56",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(ii)** \n",
    "Compute the fitness of the discovered Petri net using token-based replay.\n",
    "\n",
    "*Hint: PM4Py can directly (using the top-level API) compute the number of missing, remaining, consumed, and produced tokens. Based on these, you can, for example, compute the token-based replay fitness.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa42fb9-c4f2-4bec-a89a-308bef0b026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ab43b6-d08b-4b09-8b96-54b911b9c34f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(iii)** \n",
    "Filter the log to contain only traces where *Register Low* occurs. How many traces does the resulting log `log_low` contain?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba7573-fcc6-46a7-ae70-35111ba8e5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ef414-44a5-4a73-906f-36f673cfb6d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(iv)** \n",
    "Discover a Petri net for `log_low` and compute its fitness. How does this model differ from the model you discovered in *b(i)*? \n",
    "\n",
    "Suppose each of your produced process models is considered a 2-class classifier: provided a trace, it returns \"Yes\" if and only if the trace can be replayed by the model. Based on this perspective, how would the two process models compare in terms of precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc94f06-ad65-408d-ad9e-e8b114838b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74965d2a-c969-438d-b1f7-a8123142b789",
   "metadata": {},
   "source": [
    "**Your answer:** *(Briefly describe the difference between the two models here. About two sentences can be enough.*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9833080-54f8-4e1b-baea-810962a98281",
   "metadata": {},
   "source": [
    "**Your answer**: *(Relate your observations to precision here.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c73b63-a4a4-4438-a8e4-d2ebfe276051",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Conformance Diagnostics De-jure Model\n",
    "The process owner provides you a de-jure model (i.e., a model of the should-be process) and a slightly changed version of the so far considered event log. \n",
    "\n",
    "In this task, you will again apply conformance checking by means of token-based replay to provide diagnostics on deviations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390eabfa-faab-4bea-88ae-7c39bcad1ba0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(v)** \n",
    "Load the Petri net *pn_conf.apnml*, the event log *log_conf.xes*, and provide the overall (i.e., model-based) token-based replay fitness score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98673f6e-e70e-4bc5-969b-057e954c58e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b101e2-9b42-481f-a5f2-3c066416a131",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(vi)** \n",
    "To provide additional diagnostics on the deviations, compute the missing, consumed, produced and remaining number of tokens for **each place**. \n",
    "To do so, use the following *pm4py* code:\n",
    "    \n",
    "    from pm4py.algo.conformance.tokenreplay import algorithm as token_based_replay\n",
    "\ttbr_results, place_fitness, transition_fitness, notexisting_activities_in_model =\n",
    "    token_based_replay.apply(log_conf, net_conf, im_conf, fm_conf, parameters={\"enable_pltr_fitness\": True, \"disable_variants\": True})\n",
    "\n",
    "After running this line for log `log_conf`, Petri net `net_conf` with initial marking `im_conf` and final marking `fm_conf`, the variable `place_fitness` will contain the token counts for each place and trace. Therefore, you will only need aggregate over the traces.\n",
    "Print a table of the token counts per place. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a01c3b-4be6-4ea3-a0c7-3572d5bd696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e098ac37-4978-4520-9d39-51462a1f5038",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(vii)** \n",
    "Consider the token counts per place and a few unfitting traces, which deviation(s) do you observe? Describe the deviation and briefly explain how it can be related to the token counts of the individual places. For example, activity *xy* is often missing resulting in a high number of missing tokens in place *p*.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02586824-a2d3-4d36-b181-6e70fc3e2b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cec26df-0110-41a8-a427-eeeea611fd7c",
   "metadata": {},
   "source": [
    "**Your answer:** *(Describe the deviation(s). One sentence can already be enough.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fdc361-bfa7-49d1-809f-4509b427d8da",
   "metadata": {},
   "source": [
    "**Your answer:** (*Relate the deviation(s) to the token counts of the individual places. Roughly five sentences can be enough for a precise description.*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dcdf9d-c227-4436-b9c3-d45c5b6f7de4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### c) Analyzing Fraud (4pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2048349d-ebf9-48cc-af0b-9da5a2828b23",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **c(i)** \n",
    "Filter the event log so that it only contains traces where a fraud report is filled (occurrence of `Fill Fraud Report`). For theses traces, create a bar plot showing the number of products of a certain brand involved in the fraud. Describe the resulting plot.\n",
    "\n",
    "*Hint: Each case is associated with precisely one brand.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55a45be-a26c-4098-b23a-29b85102a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca63f184-9cb3-48a0-abd4-b4146f1ebcaa",
   "metadata": {},
   "source": [
    "**Your answer:** (*Describe the plot in two to three sentences.*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c635a50b-a7d9-433c-96c8-aae526f9e459",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **c(ii)**  \n",
    "The plot shows differences between brands. Discuss the result. Consider what you learned in Lecture 11 (association rules). Try to provide additional analysis results to underpin your discussion.\n",
    "\n",
    "*Hint: A very short additional analysis (i.e., a few lines of code) might already be sufficient.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32ba47d-c41f-418a-8b6b-b8176171b0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (for a short additional analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d49c56-6d9e-4b11-9fa4-d623f87a2f9a",
   "metadata": {},
   "source": [
    "**Your answer:** *(Relate your results to Lecture 11, approximately one short paragraph)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63da1709-2452-4ac2-a162-ea5702d1694a",
   "metadata": {},
   "source": [
    "## Question 6 - Simpson's Paradox (3 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd0ddaa-03b6-4195-923b-6ce0e73b02f0",
   "metadata": {},
   "source": [
    "### Sex Bias in Berkeley Graduate Admissions?\n",
    "\n",
    "In the Fall of 1973, the University of California at Berkeley released data about their graduate class. The data showed the major the applicant applied to, their self-reported gender (Male or Female), and whether or not they were accepted or rejected. The acceptance rates between men and women were different. This caused immediate concern in the public as people thought that Berkeley was biased against women."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df6ab4-e21f-4265-8d40-077bff5d5baa",
   "metadata": {},
   "source": [
    "The \"Berkeley Dataset\" contains all 12,763 applicants to UC-Berkeley's graduate programs in Fall 1973. This dataset was published by UC-Berkeley researchers in an analysis to understand the possible gender bias in admissions.\n",
    "\n",
    "Dataset Variables:\n",
    "\n",
    "Year : number ➜ The application year (this value is always 1973)\n",
    "\n",
    "Major : string ➜: An anonymized major code (either A, B, C, D, E, F, or Other). The specific majors are unknown except that A-F are the six majors with the most applicants in Fall 1973\n",
    "\n",
    "Gender : string ➜ Applicant self-reported gender (either M or F)\n",
    "\n",
    "Admission: string ➜ Admission decision (either Rejected or Accepted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058897ab-2f92-4f71-b00c-ec3109d63687",
   "metadata": {},
   "source": [
    "**a)**\n",
    "Upload the data from the `berkeley.csv` file and load it into a dataframe named `data`. Display the first few lines from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a34afacf-87c8-472b-86ae-728311145b18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Major</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Admission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1973</td>\n",
       "      <td>C</td>\n",
       "      <td>F</td>\n",
       "      <td>Rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1973</td>\n",
       "      <td>B</td>\n",
       "      <td>M</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1973</td>\n",
       "      <td>Other</td>\n",
       "      <td>F</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1973</td>\n",
       "      <td>Other</td>\n",
       "      <td>M</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1973</td>\n",
       "      <td>Other</td>\n",
       "      <td>M</td>\n",
       "      <td>Rejected</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year  Major Gender Admission\n",
       "0  1973      C      F  Rejected\n",
       "1  1973      B      M  Accepted\n",
       "2  1973  Other      F  Accepted\n",
       "3  1973  Other      M  Accepted\n",
       "4  1973  Other      M  Rejected"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "data = pd.read_csv(\"./datasets/berkeley.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5bd1eb-0c70-4e11-b0b9-fa132dbebdd3",
   "metadata": {},
   "source": [
    "**b)** Remove the \"Year\" column as it does not contain any information in this dataset (all years are 1973.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db47b833-2b21-473e-a2ae-93edceb1478c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Major</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Admission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C</td>\n",
       "      <td>F</td>\n",
       "      <td>Rejected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>M</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Other</td>\n",
       "      <td>F</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Other</td>\n",
       "      <td>M</td>\n",
       "      <td>Accepted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Other</td>\n",
       "      <td>M</td>\n",
       "      <td>Rejected</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Major Gender Admission\n",
       "0      C      F  Rejected\n",
       "1      B      M  Accepted\n",
       "2  Other      F  Accepted\n",
       "3  Other      M  Accepted\n",
       "4  Other      M  Rejected"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code\n",
    "data.drop([\"Year\"], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef777de3-cdcd-474d-ba69-d8c206bfd46e",
   "metadata": {},
   "source": [
    "**c)** For each of the values of column \"Gender\", compute the admission rate and compare them against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f038e712-b3d7-4e28-a809-8506d0ff8847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2a3b7-659b-4710-91b1-230e19206a82",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20439634-fed0-47fa-9ca6-68c9cf001f7c",
   "metadata": {},
   "source": [
    "**d)** For each value combination of the \"Gender\" and \"Major\" columns, compute the admission rate. Compare the admission rate of women against the admission rate of men for each of the majors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480b4533-2741-4433-a74b-8ec7bb4bb494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daa585b-c647-499b-9afb-78e1c3a2502e",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78625577-641f-4f34-a633-bae183fee886",
   "metadata": {},
   "source": [
    "**e)** Can you confirm there is a sex bias in the admission rates of the students?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf0d7eb-e486-4b0e-96e9-da24e9bc8b12",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927712ec-fd3c-4561-8732-c3babeb514f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Question 7: Big Data (15pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e67686c-0788-4b91-bcf0-70b8e8738aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193ba79e-e377-41e0-84b1-d61fc81f0bb1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "You are a data scientist at NASA, an agency for civil space programs, and with your team you develop and maintain the software of the NASA Crew Exploration Vehicle (CEV).  Your task is to analyze the performance of the software, and as a first exploratory step, you would like to **compute the mean execution times** of function calls within that software. Since the running vehicle will generate a high throughput of observable events in a stream, you decide to set up a MapReduce pipeline in Hadoop. \n",
    "\n",
    "The file **nasa-cev-software-tests.tsv** records timestamped events of the vehicle's software tests. The log contains the columns *Case*, *Activity* and *Timestamp*, denoting the case ID, the activity key (method call) and timestamp of the event record in nanoseconds, respectively. Furthermore, the log contains the columns *Lifecycle Transition* and *Execution ID*. The lifecycle transition takes either of the values *start* and *complete*, stating whether the corresponding activity (method call) in that row is being started or completed at the specified timestamp. The execution ID relates each event to a concrete method call, i.e., for each execution ID, there are exactly two entries (namely a *start* and a *complete* event) in the log."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5686ba3-56fa-4e35-9423-5374e47e7acf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### a) Plan the Maths (2pt):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c24cce6",
   "metadata": {},
   "source": [
    "The mean $\\mu_n$ over numerical values $v_1,...,v_n$ is well-known to be computed as $\\mu_n = \\frac{1}{n}\\sum_{i=1}^{n} v_i$.\\\n",
    "One may also use the alternative recursive formalization $\\mu_{n+m} = \\frac{n\\cdot\\mu_{n} + m\\cdot\\mu_{m}}{n+m}$. \\\n",
    "What is the advantage of using the alternative formalization when you think of handling streaming data or distributed data? Briefly explain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4816e8fb-3968-4f34-8307-a2544fd551dc",
   "metadata": {},
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ab1e45-757c-4dc4-a962-882fb3d6d95f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### b) Set up MapReduce (10pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d054f-512e-49a6-b935-85c80880368b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Realize the computation of the mean execution times of activities as a MapReduce job. \n",
    "You need to implement this a cascaded MapReduced job. This means that the output of the first job will serve as the input of the second job. In the first job, derive the execution times of each activity execution, i.e. the time difference between the *complete* and the *start* lifecycle transition of each activity execution. In the second step, aggregate this timing information to compute the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6653ff4-8c18-44d4-9de5-98ad8715425a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **b(i)** \n",
    "Specify the *function signatures* of the map functions and the reduce functions that you are going to use.\\\n",
    "I.e., find concrete sets to substitute $K_1, V_1, ... $ in the general signatures for map and reduce functions \\\n",
    "\n",
    "$ map:  K_1 \\times V_1 \\rightarrow (K_2 \\times V_2)^* $\\\n",
    "$ reduce: K_2 \\times (V_2)^* \\rightarrow (V_3)^*$ (or a singleton $V_3$) \n",
    "\n",
    "*Hint: You may introduce symbols to denote sets, e.g. $Act$ for the set of activities.\\\n",
    "You may also first implement the solution (b(ii)) to get an idea about the underlying signatures.\\\n",
    "Mind that you need two map and two reduce functions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d24616b-2f62-4317-950c-20956c8a50bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Your answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7511e48-b42e-4236-8232-aef6fccec140",
   "metadata": {},
   "source": [
    "#### **b(ii)**: \n",
    "Specify map functions and reduce functions to compute the mean execution time per activity as python scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b833afd8-718b-41aa-b54b-4193404477fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (nasa_mapper1.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac87097-d200-4871-a3a3-513ea6a635a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (nasa_reducer1.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191be80c-2ba2-45af-90fa-cff451bb656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (nasa_mapper2.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f15c173-dd64-4d9f-b477-c20e16ae265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code (nasa_reducer2.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2739a1-c55f-4e7e-b0d2-12bcee4f3333",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9109b6f2-cc6f-41b1-8274-f651f7b663bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### c) Run MapReduce (3pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de07da6-979f-4f2a-a331-843259636a0c",
   "metadata": {},
   "source": [
    "In the following, please use one of your team members' matriculation number as an identifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b8fbac-5766-420a-9595-423f0b092d73",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **c(i) (Randomization)**: \n",
    "Before applying your functions from the previous step to the dataset, please insert the matriculation number and run the following lines to randomly filter out a few of the traces in the event log, and continue working with the filtered log. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e139cb0-b347-46be-a070-ae9447fe010a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your matriculation number here\n",
    "matr_nb = # ... #\n",
    "\n",
    "# utility code (do not change)\n",
    "import random\n",
    "random.seed(matr_nb)\n",
    "\n",
    "full_df = pd.read_csv(\"datasets/nasa-cev-software-tests.tsv\", sep=\"\\t\")\n",
    "\n",
    "case_ids = list(set(full_df[\"Case\"].values))\n",
    "case_ids.sort()\n",
    "filtered_out_case_ids = random.sample(case_ids, 10)\n",
    "filtered_case_ids = [case_id for case_id in case_ids if case_id not in filtered_out_case_ids]\n",
    "randomized_df = full_df[full_df[\"Case\"].isin(filtered_case_ids)]\n",
    "\n",
    "randomized_df.to_csv(\"datasets/nasa-cev-software-tests-randomized-\" + str(matr_nb) + \".tsv\",\n",
    "          columns=[\"Activity\", \"Timestamp\", \"Lifecycle Transition\", \"Execution ID\"],\n",
    "          sep=\"\\t\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa8989a-1d2e-4360-888a-732a269525dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **c(ii)**:\n",
    "Now, it is time to work with the Hadoop Distributed File System (HDFS). Follow the instructions below and show your results in each step (screenshots of the command line).\n",
    "\n",
    "    1) Import the event log to your Docker engine (at /usr/local/hadoop/(your_matr_nb)-event-log/). You also need to import the python scripts, but only document the event log import here.\n",
    "    2) Upload the files to the running HDFS (at /input/(your_matr_nb)-event-log/).\n",
    "\t3) Run Hadoop commands for the MapReduce computation.\n",
    "    4) Show the final output (computed mean execution times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b0d41-bf64-43a8-a297-94da3a389f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "# your code\n",
    "# Image(filename='filename_screenshot_of_a1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca0a837-d297-4bb9-b377-d9744764fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "# Image(filename='filename_screenshot_of_a2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcbd059-5813-4bf8-95ca-a5a06c3e32c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "# Image(filename='filename_screenshot_of_a3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7141831-06ce-41a8-a6e2-c31de6e775af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "# Image(filename='filename_screenshot_of_a4')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0ee778c6a8bb9a16ad2bd3ecd6936e7814c935558e31703bb7127c12cabeec97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
